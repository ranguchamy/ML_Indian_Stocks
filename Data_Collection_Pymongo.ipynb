{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62956419-8785-42ed-82cd-bae521cccd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Collection | Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566712b-32fe-4481-8fdb-2f2359cfb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Last good working | Scrape | Sentiment Analyse | Persist in DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34c596-6c86-4f09-9832-0bd77328a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta  # Import timedelta module\n",
    "import pytz\n",
    "\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip060120241424.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "        return set(df['Keyword'].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    try:\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, database_name, collection_name):\n",
    "    ca = certifi.where()\n",
    "    #uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    uri = \"mongodb://localhost:27017\"\n",
    "\n",
    "    # Create a new client and connect to the MongoDB server\n",
    "    #client = MongoClient(uri, tlsCAFile=ca)\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    # Access the specified collection\n",
    "    collection = client[database_name][collection_name]\n",
    "\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    # Check if the paragraph_content already exists in the collection\n",
    "    existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "    return existing_record is not None\n",
    "\n",
    "def read_html_data(html_content, sentiment_keywords):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        for li_tag in soup.find_all('li', {'class': 'timeline-item'}):\n",
    "            article_body_span = li_tag.find('span', {'itemprop': 'articleBody'})\n",
    "            headline_h3 = li_tag.find('h3', {'itemprop': 'headline'})\n",
    "\n",
    "            collected_content = []\n",
    "\n",
    "            if article_body_span and not article_body_span.contents:\n",
    "                content = headline_h3.get_text(strip=True)\n",
    "                collected_content.append(content)\n",
    "            elif article_body_span:\n",
    "                collected_content.extend(ptag.get_text(strip=True) for ptag in article_body_span.find_all('p'))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            paragraph_content = ' '.join(collected_content)\n",
    "\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "    \n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "    \n",
    "                    # Use current date and time in IST\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "                    #yesterday_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\")) - timedelta(days=1)\n",
    "    \n",
    "    \n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"user123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin_02\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "    \n",
    "                    # Specify the MongoDB database and collection names\n",
    "                    database_name = \"NewsAnalytics\"\n",
    "                    collection_name = \"RawNews_Hindu\"\n",
    "                    # Insert the record into MongoDB\n",
    "                    insert_record_into_mongodb(record, database_name, collection_name)\n",
    "        print(\"Completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the HTML: {e}\")\n",
    "\n",
    "def economictimes_headlines():\n",
    "    print(\"Fetching headlines from Economic Times...\")\n",
    "    #url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-03-january-2024/article67698291.ece\"\n",
    "    #url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-02-january-2024/article67695347.ece\"\n",
    "    #url = \"https://www.thehindubusinessline.com/markets/stock-markets/share-market-nifty-sensex-live-updates-04-january-2024/article67702215.ece\"\n",
    "    #url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-05-january-2024/article67706161.ece\"\n",
    "    url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-08-january-2024/article67715949.ece\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        page_request = requests.get(url)\n",
    "        page_request.raise_for_status()\n",
    "        print(\"Status:\", page_request.status_code)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making the request: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = page_request.content\n",
    "        sentiment_keywords = load_sentiment_keywords()\n",
    "        read_html_data(data, sentiment_keywords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the page: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    economictimes_headlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1304596-05e6-430c-b420-b2bca4e5222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "update existing record with FINBERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d93636-e980-461c-a9c0-0bdd5a744441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "# MongoDB connection URI (replace with your actual URI)\n",
    "uri = \"mongodb://localhost:27017\"\n",
    "# uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "def analyze_finBERT_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    outputs = finbert(**inputs)[0]\n",
    "    sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "    return sentiment_label\n",
    "\n",
    "try:\n",
    "    with MongoClient(uri) as client:\n",
    "        # Specify the database and collection\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_Hindu\"\n",
    "        # Access the specified collection\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        # Query all records in the collection\n",
    "        all_records = collection.find()\n",
    "\n",
    "        # Get the total count of records\n",
    "        total_records = collection.count_documents({})\n",
    "\n",
    "        # Iterate over each record\n",
    "        for record in all_records:\n",
    "            if \"paragraph_content\" in record and \"FinBertScore\" not in record:\n",
    "                paragraph_content = record[\"paragraph_content\"]\n",
    "\n",
    "                # Analyze FinBERT sentiment for the paragraph_content\n",
    "                sentiment_label = analyze_finBERT_sentiment(paragraph_content)\n",
    "\n",
    "                # Format the date in the desired format\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                # Update the record with the new field \"FinBertScore\" and metadata\n",
    "                update_data = {\n",
    "                    \"$set\": {\n",
    "                        \"FinBertScore\": int(sentiment_label),\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"FIN_BERT_Admin\"\n",
    "                    }\n",
    "                }\n",
    "                collection.update_one({\"_id\": record[\"_id\"]}, update_data)\n",
    "\n",
    "                # Print statement for successful entry\n",
    "                print(f\"Processed record {record['_id']} - FinBertScore: {int(sentiment_label)}, Updated at: {current_datetime_ist}, Updated by: FIN_BERT_Admin\")\n",
    "        print(\"Completed\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d89d50-5015-45d1-88af-53cad8041d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock Edge UI path ALL news insert | excel read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974433a0-ebb0-4ddb-be54-e9ab1f8f3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip060120241424.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "        return set(df.iloc[:, 0].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    try:\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, database_name, collection_name):\n",
    "    uri = \"mongodb://localhost:27017\"\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    collection = client[database_name][collection_name]\n",
    "\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    # Check if the paragraph_content already exists in the collection\n",
    "    existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "    return existing_record is not None\n",
    "\n",
    "def read_excel_data(excel_path, sheet_name, sentiment_keywords):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            paragraph_content = row[0]  # Access content from the first column\n",
    "            sentiment_result = analyze_sentiment(paragraph_content)\n",
    "\n",
    "            if sentiment_result:\n",
    "                sentiment_data = {\n",
    "                    \"label\": sentiment_result[\"label\"],\n",
    "                    \"confidence\": sentiment_result[\"score\"]\n",
    "                }\n",
    "\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                record = {\n",
    "                    \"paragraph_content\": paragraph_content,\n",
    "                    \"sentiment\": sentiment_data,\n",
    "                    \"created_at\": current_datetime_ist,\n",
    "                    \"created_by\": \"StockEdge123\",\n",
    "                    \"deleted_at\": current_datetime_ist,\n",
    "                    \"deleted_by\": \"user789\",\n",
    "                    \"is_deleted\": False,\n",
    "                    \"is_purged\": False,\n",
    "                    \"last_scraped\": current_datetime_ist,\n",
    "                    \"purge_at\": current_datetime_ist,\n",
    "                    \"purged_by\": \"admin\",\n",
    "                    \"updated_at\": current_datetime_ist,\n",
    "                    \"updated_by\": \"user456\"\n",
    "                }\n",
    "\n",
    "                database_name = \"NewsAnalytics\"\n",
    "                collection_name = \"RawNews_StockEdge\"\n",
    "                \n",
    "                insert_record_into_mongodb(record, database_name, collection_name)\n",
    "        print(\"Completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\news\\stockedge\\scrapestockedge.xlsx\"\n",
    "    sheet_name = \"Sheet1\"\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    read_excel_data(excel_path, sheet_name, sentiment_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b9e01-dfd8-46c7-8971-e2c6f57e08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock Edge UI path excel read only for Sentiment keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec504ab-6912-43eb-8be4-466c44d4e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip060120241424.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "        return set(df.iloc[:, 0].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, collection):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists in the collection\n",
    "        existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "        return existing_record is not None\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking existing record in MongoDB: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_excel_data(excel_path, sheet_name, sentiment_keywords):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "        \n",
    "        # Connect to MongoDB\n",
    "        uri = \"mongodb://localhost:27017\"\n",
    "        client = MongoClient(uri)\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_StockEdge\"\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            paragraph_content = row[0]  # Access content from the first column\n",
    "\n",
    "            # Check if the paragraph_content contains any of the keywords\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "\n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"StockEdge123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "                    insert_record_into_mongodb(record, collection)\n",
    "\n",
    "        print(\"Completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel data: {e}\")\n",
    "    finally:\n",
    "        # Close the MongoDB connection\n",
    "        client.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\news\\stockedge\\scrapestockedge.xlsx\"\n",
    "    sheet_name = \"Sheet1\"\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    read_excel_data(excel_path, sheet_name, sentiment_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41f372-5f4a-494f-b51e-a45c64a255dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
