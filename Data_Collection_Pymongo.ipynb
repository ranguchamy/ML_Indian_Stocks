{
 "cells": [
  {
   "cell_type": "raw",
   "id": "35e31dba-8d87-4b17-bbb8-019766a510a3",
   "metadata": {},
   "source": [
    "Data Collection | Mongo DB"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e9a2938-8c70-41c4-9b90-cce939b1ee57",
   "metadata": {},
   "source": [
    "Last good working | Scrape | Sentiment Analyse | Persist in DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d34c596-6c86-4f09-9832-0bd77328a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching headlines from Economic Times...\n",
      "Status: 200\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1489 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (1489) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (544) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1251 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (1251) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta  # Import timedelta module\n",
    "import pytz\n",
    "\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip23012024.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "        return set(df['Keyword'].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    try:\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return {'label': '-99 stars', 'score': 0.0}\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return {'label': '-99 stars', 'score': 0.0}\n",
    "\n",
    "def insert_record_into_mongodb(record, database_name, collection_name):\n",
    "    ca = certifi.where()\n",
    "    #uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    uri = \"mongodb://localhost:27017\"\n",
    "\n",
    "    # Create a new client and connect to the MongoDB server\n",
    "    #client = MongoClient(uri, tlsCAFile=ca)\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    # Access the specified collection\n",
    "    collection = client[database_name][collection_name]\n",
    "\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    # Check if the paragraph_content already exists in the collection\n",
    "    existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "    return existing_record is not None\n",
    "\n",
    "def read_html_data(html_content, sentiment_keywords):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "   \n",
    "        for li_tag in soup.find_all('li', {'class': 'timeline-item'}):\n",
    "            article_body_span = li_tag.find('span', {'itemprop': 'articleBody'})\n",
    "            headline_h3 = li_tag.find('h3', {'itemprop': 'headline'})\n",
    "\n",
    "            # Use collected_content for both span and h3 tags\n",
    "            collected_content = []\n",
    "\n",
    "            if article_body_span and not article_body_span.contents:\n",
    "                # If span tag is empty, use content from h3 tag\n",
    "                content = headline_h3.get_text(strip=True)\n",
    "                collected_content.append(content)\n",
    "            elif article_body_span:\n",
    "                # Collect content from the span tag\n",
    "                collected_content.extend(ptag.get_text(strip=True) for ptag in article_body_span.find_all('p'))\n",
    "            else:\n",
    "                continue  # Move to the next iteration if both span and h3 are not found\n",
    "\n",
    "            # print(f\"Paragraph Content: {' '.join(collected_content)}\")\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            paragraph_content = ' '.join(collected_content)\n",
    "\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "    \n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "    \n",
    "                    # Use current date and time in IST\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "                    #yesterday_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\")) - timedelta(days=1)\n",
    "    \n",
    "    \n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"user123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin_02\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "    \n",
    "                    # Specify the MongoDB database and collection names\n",
    "                    database_name = \"NewsAnalytics\"\n",
    "                    collection_name = \"RawNews_Hindu\"\n",
    "                    # Insert the record into MongoDB\n",
    "                    insert_record_into_mongodb(record, database_name, collection_name)\n",
    "        print(\"Completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the HTML: {e}\")\n",
    "\n",
    "def economictimes_headlines():\n",
    "    print(\"Fetching headlines from Economic Times...\")\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-03-january-2024/article67698291.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-02-january-2024/article67695347.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-markets/share-market-nifty-sensex-live-updates-04-january-2024/article67702215.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-05-january-2024/article67706161.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-08-january-2024/article67715949.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-09-january-2024/article67720535.ece\"\n",
    "    #url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-10-january-2024/article67724321.ece\"\n",
    "    ## url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-11-january-2024/article67727041.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-12-january-2024/article67730865.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-15-january-2024/article67740526.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-16-january-2024/article67743130.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-17-january-2023/article67744658.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-18-january-2024/article67748508.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-19-january-2024/article67752291.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-20-january-2024/article67757092.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-23-january-2024/article67762366.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-24-january-2024/article67768511.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-25-january-2024/article67773490.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-update-29-january-2024/article67786136.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-30-january-2023/article67789893.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-31-january-2023/article67793554.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-1-february-2024/article67796794.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-2-february-2024/article67800793.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-5-february-2024/article67810781.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-6-february-2024/article67813870.ece\"\n",
    "    url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-7-february-2024/article67817593.ece\"\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        page_request = requests.get(url)\n",
    "        page_request.raise_for_status()\n",
    "        print(\"Status:\", page_request.status_code)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making the request: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = page_request.content\n",
    "        sentiment_keywords = load_sentiment_keywords()\n",
    "        read_html_data(data, sentiment_keywords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the page: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    economictimes_headlines()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a03ff90f-3522-43e0-8065-894bf0951ae6",
   "metadata": {},
   "source": [
    "update existing record with FINBERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d93636-e980-461c-a9c0-0bdd5a744441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record 65c3b59f663e365969373cc0 - FinBertScore: 0, Updated at: 2024-02-07 22:30:01.918044+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5a3663e365969373cc2 - FinBertScore: 2, Updated at: 2024-02-07 22:30:03.238266+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5a5663e365969373cc4 - FinBertScore: 0, Updated at: 2024-02-07 22:30:04.408208+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5a7663e365969373cc6 - FinBertScore: 0, Updated at: 2024-02-07 22:30:05.212618+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5a9663e365969373cc8 - FinBertScore: 0, Updated at: 2024-02-07 22:30:05.859865+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5ab663e365969373cca - FinBertScore: 0, Updated at: 2024-02-07 22:30:09.691078+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5ad663e365969373ccc - FinBertScore: 1, Updated at: 2024-02-07 22:30:11.567284+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5ae663e365969373cce - FinBertScore: 1, Updated at: 2024-02-07 22:30:11.933427+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5b0663e365969373cd0 - FinBertScore: 1, Updated at: 2024-02-07 22:30:12.875447+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5b2663e365969373cd2 - FinBertScore: 0, Updated at: 2024-02-07 22:30:13.627314+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5b4663e365969373cd4 - FinBertScore: 0, Updated at: 2024-02-07 22:30:14.768214+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5b5663e365969373cd6 - FinBertScore: 0, Updated at: 2024-02-07 22:30:15.421237+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5b7663e365969373cd8 - FinBertScore: 0, Updated at: 2024-02-07 22:30:16.173793+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5b9663e365969373cda - FinBertScore: 2, Updated at: 2024-02-07 22:30:16.713392+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5bb663e365969373cdc - FinBertScore: 2, Updated at: 2024-02-07 22:30:17.127726+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5bd663e365969373cde - FinBertScore: 2, Updated at: 2024-02-07 22:30:17.747273+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5bf663e365969373ce0 - FinBertScore: 0, Updated at: 2024-02-07 22:30:18.396644+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5c0663e365969373ce2 - FinBertScore: 0, Updated at: 2024-02-07 22:30:19.131421+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5c2663e365969373ce4 - FinBertScore: 0, Updated at: 2024-02-07 22:30:20.004449+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5c3663e365969373ce6 - FinBertScore: 2, Updated at: 2024-02-07 22:30:20.670994+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5c5663e365969373ce8 - FinBertScore: 0, Updated at: 2024-02-07 22:30:21.338506+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5c9663e365969373cea - FinBertScore: 0, Updated at: 2024-02-07 22:30:23.076052+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5cb663e365969373cec - FinBertScore: 0, Updated at: 2024-02-07 22:30:24.269072+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5cc663e365969373cee - FinBertScore: 1, Updated at: 2024-02-07 22:30:25.078047+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5cf663e365969373cf0 - FinBertScore: 1, Updated at: 2024-02-07 22:30:27.416137+05:30, Updated by: FIN_BERT_Admin\n",
      "Error analyzing FinBERT sentiment: The size of tensor a (1559) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Processed record 65c3b5d0663e365969373cf2 - FinBertScore: 0, Updated at: 2024-02-07 22:30:27.596251+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5d1663e365969373cf4 - FinBertScore: 2, Updated at: 2024-02-07 22:30:28.040746+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5d3663e365969373cf6 - FinBertScore: 0, Updated at: 2024-02-07 22:30:28.657422+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5d4663e365969373cf8 - FinBertScore: 0, Updated at: 2024-02-07 22:30:29.045220+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5d6663e365969373cfa - FinBertScore: 1, Updated at: 2024-02-07 22:30:29.533998+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5d7663e365969373cfc - FinBertScore: 0, Updated at: 2024-02-07 22:30:30.105656+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5d9663e365969373cfe - FinBertScore: 1, Updated at: 2024-02-07 22:30:31.612799+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5db663e365969373d00 - FinBertScore: 1, Updated at: 2024-02-07 22:30:32.331353+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5dc663e365969373d02 - FinBertScore: 1, Updated at: 2024-02-07 22:30:32.685206+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5df663e365969373d04 - FinBertScore: 0, Updated at: 2024-02-07 22:30:33.460662+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5e1663e365969373d06 - FinBertScore: 1, Updated at: 2024-02-07 22:30:34.071458+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5e3663e365969373d08 - FinBertScore: 1, Updated at: 2024-02-07 22:30:34.480877+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5e7663e365969373d0a - FinBertScore: 1, Updated at: 2024-02-07 22:30:36.002914+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5ea663e365969373d0c - FinBertScore: 0, Updated at: 2024-02-07 22:30:36.519694+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5ec663e365969373d0e - FinBertScore: 1, Updated at: 2024-02-07 22:30:37.098753+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5ef663e365969373d10 - FinBertScore: 1, Updated at: 2024-02-07 22:30:37.512986+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5f1663e365969373d12 - FinBertScore: 0, Updated at: 2024-02-07 22:30:38.025621+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5f8663e365969373d14 - FinBertScore: 1, Updated at: 2024-02-07 22:30:39.729585+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5fc663e365969373d16 - FinBertScore: 0, Updated at: 2024-02-07 22:30:40.464737+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b5ff663e365969373d18 - FinBertScore: 0, Updated at: 2024-02-07 22:30:41.409021+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b601663e365969373d1a - FinBertScore: 0, Updated at: 2024-02-07 22:30:41.618913+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b603663e365969373d1c - FinBertScore: 0, Updated at: 2024-02-07 22:30:41.896039+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b605663e365969373d1e - FinBertScore: 0, Updated at: 2024-02-07 22:30:42.432339+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b606663e365969373d20 - FinBertScore: 0, Updated at: 2024-02-07 22:30:43.262694+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b608663e365969373d22 - FinBertScore: 2, Updated at: 2024-02-07 22:30:43.606248+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b60a663e365969373d24 - FinBertScore: 1, Updated at: 2024-02-07 22:30:44.719096+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b60d663e365969373d26 - FinBertScore: 1, Updated at: 2024-02-07 22:30:45.589144+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b611663e365969373d28 - FinBertScore: 2, Updated at: 2024-02-07 22:30:46.241852+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b614663e365969373d2a - FinBertScore: 2, Updated at: 2024-02-07 22:30:46.949853+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b616663e365969373d2c - FinBertScore: 1, Updated at: 2024-02-07 22:30:47.808023+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b619663e365969373d2e - FinBertScore: 1, Updated at: 2024-02-07 22:30:48.502240+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b61a663e365969373d30 - FinBertScore: 2, Updated at: 2024-02-07 22:30:50.580364+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b61d663e365969373d32 - FinBertScore: 1, Updated at: 2024-02-07 22:30:51.940392+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b620663e365969373d34 - FinBertScore: 1, Updated at: 2024-02-07 22:30:52.666362+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b623663e365969373d36 - FinBertScore: 1, Updated at: 2024-02-07 22:30:53.520631+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b625663e365969373d38 - FinBertScore: 2, Updated at: 2024-02-07 22:30:54.023058+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b628663e365969373d3a - FinBertScore: 2, Updated at: 2024-02-07 22:30:54.717113+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b62a663e365969373d3c - FinBertScore: 1, Updated at: 2024-02-07 22:30:55.370048+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b62c663e365969373d3e - FinBertScore: 1, Updated at: 2024-02-07 22:30:55.954687+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b630663e365969373d40 - FinBertScore: 1, Updated at: 2024-02-07 22:30:56.900659+05:30, Updated by: FIN_BERT_Admin\n",
      "Error analyzing FinBERT sentiment: The size of tensor a (1164) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Processed record 65c3b632663e365969373d42 - FinBertScore: 0, Updated at: 2024-02-07 22:30:56.968715+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b634663e365969373d44 - FinBertScore: 1, Updated at: 2024-02-07 22:30:57.302710+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c3b636663e365969373d46 - FinBertScore: 1, Updated at: 2024-02-07 22:30:57.722932+05:30, Updated by: FIN_BERT_Admin\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "# MongoDB connection URI (replace with your actual URI)\n",
    "uri = \"mongodb://localhost:27017\"\n",
    "# uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# def analyze_finBERT_sentiment(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "#     outputs = finbert(**inputs)[0]\n",
    "#     sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "#     return sentiment_label\n",
    "\n",
    "def analyze_finBERT_sentiment(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        outputs = finbert(**inputs)[0]\n",
    "        sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "        return sentiment_label\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing FinBERT sentiment: {e}\")\n",
    "        return 0\n",
    "\n",
    "try:\n",
    "    with MongoClient(uri) as client:\n",
    "        # Specify the database and collection\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_Hindu\"\n",
    "        # Access the specified collection\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        # Query all records in the collection\n",
    "        all_records = collection.find()\n",
    "\n",
    "        # Get the total count of records\n",
    "        total_records = collection.count_documents({})\n",
    "\n",
    "        # Iterate over each record\n",
    "        for record in all_records:\n",
    "            if \"paragraph_content\" in record and \"FinBertScore\" not in record:\n",
    "                paragraph_content = record[\"paragraph_content\"]\n",
    "\n",
    "                # Analyze FinBERT sentiment for the paragraph_content\n",
    "                sentiment_label = analyze_finBERT_sentiment(paragraph_content)\n",
    "                # Format the date in the desired format\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                # Update the record with the new field \"FinBertScore\" and metadata\n",
    "                update_data = {\n",
    "                    \"$set\": {\n",
    "                        \"FinBertScore\": int(sentiment_label),\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"FIN_BERT_Admin\"\n",
    "                    }\n",
    "                }\n",
    "                collection.update_one({\"_id\": record[\"_id\"]}, update_data)\n",
    "\n",
    "                # Print statement for successful entry\n",
    "                print(f\"Processed record {record['_id']} - FinBertScore: {int(sentiment_label)}, Updated at: {current_datetime_ist}, Updated by: FIN_BERT_Admin\")\n",
    "        print(\"Completed\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2f3b3a5-3962-43ee-96a1-dcc30cf15003",
   "metadata": {},
   "source": [
    "Stock Edge UI path excel read only for Sentiment keywords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78b45bba-86c7-4c9c-b306-099b4ddda7e9",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip090120241753.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "        return set(df.iloc[:, 0].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, collection):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists in the collection\n",
    "        existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "        return existing_record is not None\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking existing record in MongoDB: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_excel_data(excel_path, sheet_name, sentiment_keywords):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "        \n",
    "        # Connect to MongoDB\n",
    "        uri = \"mongodb://localhost:27017\"\n",
    "        client = MongoClient(uri)\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_StockEdge\"\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            paragraph_content = row[0]  # Access content from the first column\n",
    "\n",
    "            # Check if the paragraph_content contains any of the keywords\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "\n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"StockEdge123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "                    insert_record_into_mongodb(record, collection)\n",
    "\n",
    "        print(\"Completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel data: {e}\")\n",
    "    finally:\n",
    "        # Close the MongoDB connection\n",
    "        client.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\news\\stockedge\\scrapestockedge.xlsx\"\n",
    "    sheet_name = \"Sheet1\"\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    read_excel_data(excel_path, sheet_name, sentiment_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41f372-5f4a-494f-b51e-a45c64a255dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
