{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62956419-8785-42ed-82cd-bae521cccd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Collection | Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566712b-32fe-4481-8fdb-2f2359cfb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Last good working | Scrape | Sentiment Analyse | Persist in DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d34c596-6c86-4f09-9832-0bd77328a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching headlines from Economic Times...\n",
      "Status: 200\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1788 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (1788) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (623) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (875 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (875) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta  # Import timedelta module\n",
    "import pytz\n",
    "\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip23012024.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "        return set(df['Keyword'].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    try:\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return {'label': '-99 stars', 'score': 0.0}\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return {'label': '-99 stars', 'score': 0.0}\n",
    "\n",
    "def insert_record_into_mongodb(record, database_name, collection_name):\n",
    "    ca = certifi.where()\n",
    "    #uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    uri = \"mongodb://localhost:27017\"\n",
    "\n",
    "    # Create a new client and connect to the MongoDB server\n",
    "    #client = MongoClient(uri, tlsCAFile=ca)\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    # Access the specified collection\n",
    "    collection = client[database_name][collection_name]\n",
    "\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    # Check if the paragraph_content already exists in the collection\n",
    "    existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "    return existing_record is not None\n",
    "\n",
    "def read_html_data(html_content, sentiment_keywords):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "   \n",
    "        for li_tag in soup.find_all('li', {'class': 'timeline-item'}):\n",
    "            article_body_span = li_tag.find('span', {'itemprop': 'articleBody'})\n",
    "            headline_h3 = li_tag.find('h3', {'itemprop': 'headline'})\n",
    "\n",
    "            # Use collected_content for both span and h3 tags\n",
    "            collected_content = []\n",
    "\n",
    "            if article_body_span and not article_body_span.contents:\n",
    "                # If span tag is empty, use content from h3 tag\n",
    "                content = headline_h3.get_text(strip=True)\n",
    "                collected_content.append(content)\n",
    "            elif article_body_span:\n",
    "                # Collect content from the span tag\n",
    "                collected_content.extend(ptag.get_text(strip=True) for ptag in article_body_span.find_all('p'))\n",
    "            else:\n",
    "                continue  # Move to the next iteration if both span and h3 are not found\n",
    "\n",
    "            # print(f\"Paragraph Content: {' '.join(collected_content)}\")\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            paragraph_content = ' '.join(collected_content)\n",
    "\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "    \n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "    \n",
    "                    # Use current date and time in IST\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "                    #yesterday_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\")) - timedelta(days=1)\n",
    "    \n",
    "    \n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"user123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin_02\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "    \n",
    "                    # Specify the MongoDB database and collection names\n",
    "                    database_name = \"NewsAnalytics\"\n",
    "                    collection_name = \"RawNews_Hindu\"\n",
    "                    # Insert the record into MongoDB\n",
    "                    insert_record_into_mongodb(record, database_name, collection_name)\n",
    "        print(\"Completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the HTML: {e}\")\n",
    "\n",
    "def economictimes_headlines():\n",
    "    print(\"Fetching headlines from Economic Times...\")\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-03-january-2024/article67698291.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-02-january-2024/article67695347.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-markets/share-market-nifty-sensex-live-updates-04-january-2024/article67702215.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-05-january-2024/article67706161.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-08-january-2024/article67715949.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-09-january-2024/article67720535.ece\"\n",
    "    #url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-10-january-2024/article67724321.ece\"\n",
    "    ## url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-11-january-2024/article67727041.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-12-january-2024/article67730865.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-15-january-2024/article67740526.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-16-january-2024/article67743130.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-17-january-2023/article67744658.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-18-january-2024/article67748508.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-19-january-2024/article67752291.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-20-january-2024/article67757092.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-23-january-2024/article67762366.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-24-january-2024/article67768511.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-25-january-2024/article67773490.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-update-29-january-2024/article67786136.ece\"\n",
    "    url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-30-january-2023/article67789893.ece\"\n",
    "\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        page_request = requests.get(url)\n",
    "        page_request.raise_for_status()\n",
    "        print(\"Status:\", page_request.status_code)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making the request: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = page_request.content\n",
    "        sentiment_keywords = load_sentiment_keywords()\n",
    "        read_html_data(data, sentiment_keywords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the page: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    economictimes_headlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1304596-05e6-430c-b420-b2bca4e5222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "update existing record with FINBERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d93636-e980-461c-a9c0-0bdd5a744441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record 65b8ee7468f9c941d4102d8c - FinBertScore: 0, Updated at: 2024-01-30 18:13:40.838288+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee7668f9c941d4102d8e - FinBertScore: 0, Updated at: 2024-01-30 18:13:40.925845+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee7768f9c941d4102d90 - FinBertScore: 0, Updated at: 2024-01-30 18:13:41.035380+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee7968f9c941d4102d92 - FinBertScore: 0, Updated at: 2024-01-30 18:13:41.128957+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee7a68f9c941d4102d94 - FinBertScore: 0, Updated at: 2024-01-30 18:13:41.248557+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee7e68f9c941d4102d96 - FinBertScore: 1, Updated at: 2024-01-30 18:13:41.340081+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee7f68f9c941d4102d98 - FinBertScore: 1, Updated at: 2024-01-30 18:13:41.443055+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8168f9c941d4102d9a - FinBertScore: 0, Updated at: 2024-01-30 18:13:41.518587+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8268f9c941d4102d9c - FinBertScore: 1, Updated at: 2024-01-30 18:13:41.617421+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8368f9c941d4102d9e - FinBertScore: 0, Updated at: 2024-01-30 18:13:41.726000+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8468f9c941d4102da0 - FinBertScore: 0, Updated at: 2024-01-30 18:13:41.820748+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8568f9c941d4102da2 - FinBertScore: 0, Updated at: 2024-01-30 18:13:41.934190+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8768f9c941d4102da4 - FinBertScore: 0, Updated at: 2024-01-30 18:13:42.037732+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8868f9c941d4102da6 - FinBertScore: 0, Updated at: 2024-01-30 18:13:42.143287+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8968f9c941d4102da8 - FinBertScore: 0, Updated at: 2024-01-30 18:13:42.246014+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8a68f9c941d4102daa - FinBertScore: 1, Updated at: 2024-01-30 18:13:42.348639+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8b68f9c941d4102dac - FinBertScore: 0, Updated at: 2024-01-30 18:13:42.496885+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8c68f9c941d4102dae - FinBertScore: 0, Updated at: 2024-01-30 18:13:42.639633+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8d68f9c941d4102db0 - FinBertScore: 0, Updated at: 2024-01-30 18:13:42.728490+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8e68f9c941d4102db2 - FinBertScore: 0, Updated at: 2024-01-30 18:13:42.823828+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee8f68f9c941d4102db4 - FinBertScore: 0, Updated at: 2024-01-30 18:13:42.948389+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9168f9c941d4102db6 - FinBertScore: 0, Updated at: 2024-01-30 18:13:43.059485+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9268f9c941d4102db8 - FinBertScore: 0, Updated at: 2024-01-30 18:13:43.177073+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9368f9c941d4102dba - FinBertScore: 0, Updated at: 2024-01-30 18:13:43.285580+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9568f9c941d4102dbc - FinBertScore: 0, Updated at: 2024-01-30 18:13:43.391111+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9668f9c941d4102dbe - FinBertScore: 0, Updated at: 2024-01-30 18:13:43.508741+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9768f9c941d4102dc0 - FinBertScore: 0, Updated at: 2024-01-30 18:13:43.605346+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9868f9c941d4102dc2 - FinBertScore: 0, Updated at: 2024-01-30 18:13:43.708152+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9968f9c941d4102dc4 - FinBertScore: 0, Updated at: 2024-01-30 18:13:43.809709+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9a68f9c941d4102dc6 - FinBertScore: 0, Updated at: 2024-01-30 18:13:43.919286+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9b68f9c941d4102dc8 - FinBertScore: 0, Updated at: 2024-01-30 18:13:44.023872+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9d68f9c941d4102dca - FinBertScore: 0, Updated at: 2024-01-30 18:13:44.138400+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9e68f9c941d4102dcc - FinBertScore: 0, Updated at: 2024-01-30 18:13:44.246009+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8ee9f68f9c941d4102dce - FinBertScore: 2, Updated at: 2024-01-30 18:13:44.372083+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8eea068f9c941d4102dd0 - FinBertScore: 0, Updated at: 2024-01-30 18:13:44.483612+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8eea168f9c941d4102dd2 - FinBertScore: 0, Updated at: 2024-01-30 18:13:44.591154+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8eea268f9c941d4102dd4 - FinBertScore: 0, Updated at: 2024-01-30 18:13:44.703780+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b8eea368f9c941d4102dd6 - FinBertScore: 0, Updated at: 2024-01-30 18:13:44.806360+05:30, Updated by: FIN_BERT_Admin\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "# MongoDB connection URI (replace with your actual URI)\n",
    "uri = \"mongodb://localhost:27017\"\n",
    "# uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# def analyze_finBERT_sentiment(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "#     outputs = finbert(**inputs)[0]\n",
    "#     sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "#     return sentiment_label\n",
    "\n",
    "def analyze_finBERT_sentiment(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        outputs = finbert(**inputs)[0]\n",
    "        sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "        return sentiment_label\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing FinBERT sentiment: {e}\")\n",
    "        return 0\n",
    "\n",
    "try:\n",
    "    with MongoClient(uri) as client:\n",
    "        # Specify the database and collection\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_StockEdge\"\n",
    "        # Access the specified collection\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        # Query all records in the collection\n",
    "        all_records = collection.find()\n",
    "\n",
    "        # Get the total count of records\n",
    "        total_records = collection.count_documents({})\n",
    "\n",
    "        # Iterate over each record\n",
    "        for record in all_records:\n",
    "            if \"paragraph_content\" in record and \"FinBertScore\" not in record:\n",
    "                paragraph_content = record[\"paragraph_content\"]\n",
    "\n",
    "                # Analyze FinBERT sentiment for the paragraph_content\n",
    "                sentiment_label = analyze_finBERT_sentiment(paragraph_content)\n",
    "                # Format the date in the desired format\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                # Update the record with the new field \"FinBertScore\" and metadata\n",
    "                update_data = {\n",
    "                    \"$set\": {\n",
    "                        \"FinBertScore\": int(sentiment_label),\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"FIN_BERT_Admin\"\n",
    "                    }\n",
    "                }\n",
    "                collection.update_one({\"_id\": record[\"_id\"]}, update_data)\n",
    "\n",
    "                # Print statement for successful entry\n",
    "                print(f\"Processed record {record['_id']} - FinBertScore: {int(sentiment_label)}, Updated at: {current_datetime_ist}, Updated by: FIN_BERT_Admin\")\n",
    "        print(\"Completed\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d89d50-5015-45d1-88af-53cad8041d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock Edge UI path ALL news insert | excel read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974433a0-ebb0-4ddb-be54-e9ab1f8f3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip23012024.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "        return set(df.iloc[:, 0].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    try:\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, database_name, collection_name):\n",
    "    uri = \"mongodb://localhost:27017\"\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    collection = client[database_name][collection_name]\n",
    "\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    # Check if the paragraph_content already exists in the collection\n",
    "    existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "    return existing_record is not None\n",
    "\n",
    "def read_excel_data(excel_path, sheet_name, sentiment_keywords):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            paragraph_content = row[0]  # Access content from the first column\n",
    "            sentiment_result = analyze_sentiment(paragraph_content)\n",
    "\n",
    "            if sentiment_result:\n",
    "                sentiment_data = {\n",
    "                    \"label\": sentiment_result[\"label\"],\n",
    "                    \"confidence\": sentiment_result[\"score\"]\n",
    "                }\n",
    "\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                record = {\n",
    "                    \"paragraph_content\": paragraph_content,\n",
    "                    \"sentiment\": sentiment_data,\n",
    "                    \"created_at\": current_datetime_ist,\n",
    "                    \"created_by\": \"StockEdge123\",\n",
    "                    \"deleted_at\": current_datetime_ist,\n",
    "                    \"deleted_by\": \"user789\",\n",
    "                    \"is_deleted\": False,\n",
    "                    \"is_purged\": False,\n",
    "                    \"last_scraped\": current_datetime_ist,\n",
    "                    \"purge_at\": current_datetime_ist,\n",
    "                    \"purged_by\": \"admin\",\n",
    "                    \"updated_at\": current_datetime_ist,\n",
    "                    \"updated_by\": \"user456\"\n",
    "                }\n",
    "\n",
    "                database_name = \"NewsAnalytics\"\n",
    "                collection_name = \"RawNews_StockEdge\"\n",
    "                \n",
    "                insert_record_into_mongodb(record, database_name, collection_name)\n",
    "        print(\"Completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\news\\stockedge\\scrapestockedge.xlsx\"\n",
    "    sheet_name = \"Sheet1\"\n",
    "    # sentiment_keywords = load_sentiment_keywords()\n",
    "    read_excel_data(excel_path, sheet_name, sentiment_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b9e01-dfd8-46c7-8971-e2c6f57e08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock Edge UI path excel read only for Sentiment keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec504ab-6912-43eb-8be4-466c44d4e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip090120241753.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "        return set(df.iloc[:, 0].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, collection):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists in the collection\n",
    "        existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "        return existing_record is not None\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking existing record in MongoDB: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_excel_data(excel_path, sheet_name, sentiment_keywords):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "        \n",
    "        # Connect to MongoDB\n",
    "        uri = \"mongodb://localhost:27017\"\n",
    "        client = MongoClient(uri)\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_StockEdge\"\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            paragraph_content = row[0]  # Access content from the first column\n",
    "\n",
    "            # Check if the paragraph_content contains any of the keywords\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "\n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"StockEdge123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "                    insert_record_into_mongodb(record, collection)\n",
    "\n",
    "        print(\"Completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel data: {e}\")\n",
    "    finally:\n",
    "        # Close the MongoDB connection\n",
    "        client.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\news\\stockedge\\scrapestockedge.xlsx\"\n",
    "    sheet_name = \"Sheet1\"\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    read_excel_data(excel_path, sheet_name, sentiment_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41f372-5f4a-494f-b51e-a45c64a255dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
