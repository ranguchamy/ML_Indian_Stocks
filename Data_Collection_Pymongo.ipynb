{
 "cells": [
  {
   "cell_type": "raw",
   "id": "35e31dba-8d87-4b17-bbb8-019766a510a3",
   "metadata": {},
   "source": [
    "Data Collection | Mongo DB"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e9a2938-8c70-41c4-9b90-cce939b1ee57",
   "metadata": {},
   "source": [
    "Last good working | Scrape | Sentiment Analyse | Persist in DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d34c596-6c86-4f09-9832-0bd77328a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching headlines from Economic Times...\n",
      "Status: 200\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (773) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (680 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (680) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record with paragraph_content already exists. Skipping insertion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1982 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (1982) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta  # Import timedelta module\n",
    "import pytz\n",
    "\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip23012024.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "        return set(df['Keyword'].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    try:\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return {'label': '-99 stars', 'score': 0.0}\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return {'label': '-99 stars', 'score': 0.0}\n",
    "\n",
    "def insert_record_into_mongodb(record, database_name, collection_name):\n",
    "    ca = certifi.where()\n",
    "    #uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    uri = \"mongodb://localhost:27017\"\n",
    "\n",
    "    # Create a new client and connect to the MongoDB server\n",
    "    #client = MongoClient(uri, tlsCAFile=ca)\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    # Access the specified collection\n",
    "    collection = client[database_name][collection_name]\n",
    "\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    # Check if the paragraph_content already exists in the collection\n",
    "    existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "    return existing_record is not None\n",
    "\n",
    "def read_html_data(html_content, sentiment_keywords):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "   \n",
    "        for li_tag in soup.find_all('li', {'class': 'timeline-item'}):\n",
    "            article_body_span = li_tag.find('span', {'itemprop': 'articleBody'})\n",
    "            headline_h3 = li_tag.find('h3', {'itemprop': 'headline'})\n",
    "\n",
    "            # Use collected_content for both span and h3 tags\n",
    "            collected_content = []\n",
    "\n",
    "            if article_body_span and not article_body_span.contents:\n",
    "                # If span tag is empty, use content from h3 tag\n",
    "                content = headline_h3.get_text(strip=True)\n",
    "                collected_content.append(content)\n",
    "            elif article_body_span:\n",
    "                # Collect content from the span tag\n",
    "                collected_content.extend(ptag.get_text(strip=True) for ptag in article_body_span.find_all('p'))\n",
    "            else:\n",
    "                continue  # Move to the next iteration if both span and h3 are not found\n",
    "\n",
    "            # print(f\"Paragraph Content: {' '.join(collected_content)}\")\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            paragraph_content = ' '.join(collected_content)\n",
    "\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "    \n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "    \n",
    "                    # Use current date and time in IST\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "                    #yesterday_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\")) - timedelta(days=1)\n",
    "    \n",
    "    \n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"user123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin_02\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "    \n",
    "                    # Specify the MongoDB database and collection names\n",
    "                    database_name = \"NewsAnalytics\"\n",
    "                    collection_name = \"RawNews_Hindu\"\n",
    "                    # Insert the record into MongoDB\n",
    "                    insert_record_into_mongodb(record, database_name, collection_name)\n",
    "        print(\"Completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the HTML: {e}\")\n",
    "\n",
    "def economictimes_headlines():\n",
    "    print(\"Fetching headlines from Economic Times...\")\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-03-january-2024/article67698291.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-02-january-2024/article67695347.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-markets/share-market-nifty-sensex-live-updates-04-january-2024/article67702215.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-05-january-2024/article67706161.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-08-january-2024/article67715949.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-09-january-2024/article67720535.ece\"\n",
    "    #url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-10-january-2024/article67724321.ece\"\n",
    "    ## url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-11-january-2024/article67727041.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-12-january-2024/article67730865.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-15-january-2024/article67740526.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-16-january-2024/article67743130.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-17-january-2023/article67744658.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-18-january-2024/article67748508.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-19-january-2024/article67752291.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-20-january-2024/article67757092.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-23-january-2024/article67762366.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-24-january-2024/article67768511.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-25-january-2024/article67773490.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-update-29-january-2024/article67786136.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-30-january-2023/article67789893.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-31-january-2023/article67793554.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-1-february-2024/article67796794.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-2-february-2024/article67800793.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-5-february-2024/article67810781.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-6-february-2024/article67813870.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-7-february-2024/article67817593.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-08-february-2024/article67821745.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-9-february-2024/article67825410.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-12-february-2024/article67836096.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-13-february-2024/article67838229.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-14-february-2024/article67842737.ece\"\n",
    "\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-15-february-2024/article67845496.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-16-february-2024/article67849197.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-updates-19-february-2024/article67860834.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-updates-20-february-2024/article67863390.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-21-february-2024/article67867142.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-22-february-2024/article67871508.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-23-february-2024/article67875397.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-26-february-2024/article67885065.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-27-february-2024/article67888298.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-28-february-2024/article67891443.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-29-february-2024/article67895848.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-01-march-2024/article67899821.ece\"\n",
    "    url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-4-march-2024/article67909983.ece\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        page_request = requests.get(url)\n",
    "        page_request.raise_for_status()\n",
    "        print(\"Status:\", page_request.status_code)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making the request: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = page_request.content\n",
    "        sentiment_keywords = load_sentiment_keywords()\n",
    "        read_html_data(data, sentiment_keywords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the page: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    economictimes_headlines()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a03ff90f-3522-43e0-8065-894bf0951ae6",
   "metadata": {},
   "source": [
    "update existing record with FINBERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d93636-e980-461c-a9c0-0bdd5a744441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record 65e561b6cf427725b1f03cfb - FinBertScore: 0, Updated at: 2024-03-04 11:23:54.630522+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65e561b8cf427725b1f03cfd - FinBertScore: 0, Updated at: 2024-03-04 11:23:54.777523+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65e561bacf427725b1f03cff - FinBertScore: 0, Updated at: 2024-03-04 11:23:54.972905+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65e561bccf427725b1f03d01 - FinBertScore: 0, Updated at: 2024-03-04 11:23:55.155208+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65e561becf427725b1f03d03 - FinBertScore: 0, Updated at: 2024-03-04 11:23:55.507084+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65e561c0cf427725b1f03d05 - FinBertScore: 1, Updated at: 2024-03-04 11:23:55.860758+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65e561c4cf427725b1f03d07 - FinBertScore: 0, Updated at: 2024-03-04 11:23:56.271022+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65e561c7cf427725b1f03d09 - FinBertScore: 0, Updated at: 2024-03-04 11:23:56.499963+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65e561c9cf427725b1f03d0b - FinBertScore: 1, Updated at: 2024-03-04 11:23:56.702242+05:30, Updated by: FIN_BERT_Admin\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "# MongoDB connection URI (replace with your actual URI)\n",
    "uri = \"mongodb://localhost:27017\"\n",
    "# uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# def analyze_finBERT_sentiment(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "#     outputs = finbert(**inputs)[0]\n",
    "#     sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "#     return sentiment_label\n",
    "\n",
    "def analyze_finBERT_sentiment(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        outputs = finbert(**inputs)[0]\n",
    "        sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "        return sentiment_label\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing FinBERT sentiment: {e}\")\n",
    "        return 0\n",
    "\n",
    "try:\n",
    "    with MongoClient(uri) as client:\n",
    "        # Specify the database and collection\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_Hindu\"\n",
    "        # Access the specified collection\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        # Query all records in the collection\n",
    "        all_records = collection.find()\n",
    "\n",
    "        # Get the total count of records\n",
    "        total_records = collection.count_documents({})\n",
    "\n",
    "        # Iterate over each record\n",
    "        for record in all_records:\n",
    "            if \"paragraph_content\" in record and \"FinBertScore\" not in record:\n",
    "                paragraph_content = record[\"paragraph_content\"]\n",
    "\n",
    "                # Analyze FinBERT sentiment for the paragraph_content\n",
    "                sentiment_label = analyze_finBERT_sentiment(paragraph_content)\n",
    "                # Format the date in the desired format\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                # Update the record with the new field \"FinBertScore\" and metadata\n",
    "                update_data = {\n",
    "                    \"$set\": {\n",
    "                        \"FinBertScore\": int(sentiment_label),\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"FIN_BERT_Admin\"\n",
    "                    }\n",
    "                }\n",
    "                collection.update_one({\"_id\": record[\"_id\"]}, update_data)\n",
    "\n",
    "                # Print statement for successful entry\n",
    "                print(f\"Processed record {record['_id']} - FinBertScore: {int(sentiment_label)}, Updated at: {current_datetime_ist}, Updated by: FIN_BERT_Admin\")\n",
    "        print(\"Completed\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2f3b3a5-3962-43ee-96a1-dcc30cf15003",
   "metadata": {},
   "source": [
    "Stock Edge UI path excel read only for Sentiment keywords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78b45bba-86c7-4c9c-b306-099b4ddda7e9",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip090120241753.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "        return set(df.iloc[:, 0].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, collection):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists in the collection\n",
    "        existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "        return existing_record is not None\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking existing record in MongoDB: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_excel_data(excel_path, sheet_name, sentiment_keywords):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "        \n",
    "        # Connect to MongoDB\n",
    "        uri = \"mongodb://localhost:27017\"\n",
    "        client = MongoClient(uri)\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_StockEdge\"\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            paragraph_content = row[0]  # Access content from the first column\n",
    "\n",
    "            # Check if the paragraph_content contains any of the keywords\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "\n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"StockEdge123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "                    insert_record_into_mongodb(record, collection)\n",
    "\n",
    "        print(\"Completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel data: {e}\")\n",
    "    finally:\n",
    "        # Close the MongoDB connection\n",
    "        client.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\news\\stockedge\\scrapestockedge.xlsx\"\n",
    "    sheet_name = \"Sheet1\"\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    read_excel_data(excel_path, sheet_name, sentiment_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41f372-5f4a-494f-b51e-a45c64a255dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
