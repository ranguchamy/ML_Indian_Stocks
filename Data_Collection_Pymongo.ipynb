{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62956419-8785-42ed-82cd-bae521cccd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Collection | Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566712b-32fe-4481-8fdb-2f2359cfb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Last good working | Scrape | Sentiment Analyse | Persist in DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d34c596-6c86-4f09-9832-0bd77328a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching headlines from Economic Times...\n",
      "Status: 200\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1639 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (1639) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (628) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (905 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (905) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (820 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (820) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1175 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing sentiment: The size of tensor a (1175) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta  # Import timedelta module\n",
    "import pytz\n",
    "\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip23012024.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "        return set(df['Keyword'].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    try:\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return {'label': '-99 stars', 'score': 0.0}\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return {'label': '-99 stars', 'score': 0.0}\n",
    "\n",
    "def insert_record_into_mongodb(record, database_name, collection_name):\n",
    "    ca = certifi.where()\n",
    "    #uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    uri = \"mongodb://localhost:27017\"\n",
    "\n",
    "    # Create a new client and connect to the MongoDB server\n",
    "    #client = MongoClient(uri, tlsCAFile=ca)\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    # Access the specified collection\n",
    "    collection = client[database_name][collection_name]\n",
    "\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    # Check if the paragraph_content already exists in the collection\n",
    "    existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "    return existing_record is not None\n",
    "\n",
    "def read_html_data(html_content, sentiment_keywords):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "   \n",
    "        for li_tag in soup.find_all('li', {'class': 'timeline-item'}):\n",
    "            article_body_span = li_tag.find('span', {'itemprop': 'articleBody'})\n",
    "            headline_h3 = li_tag.find('h3', {'itemprop': 'headline'})\n",
    "\n",
    "            # Use collected_content for both span and h3 tags\n",
    "            collected_content = []\n",
    "\n",
    "            if article_body_span and not article_body_span.contents:\n",
    "                # If span tag is empty, use content from h3 tag\n",
    "                content = headline_h3.get_text(strip=True)\n",
    "                collected_content.append(content)\n",
    "            elif article_body_span:\n",
    "                # Collect content from the span tag\n",
    "                collected_content.extend(ptag.get_text(strip=True) for ptag in article_body_span.find_all('p'))\n",
    "            else:\n",
    "                continue  # Move to the next iteration if both span and h3 are not found\n",
    "\n",
    "            # print(f\"Paragraph Content: {' '.join(collected_content)}\")\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            paragraph_content = ' '.join(collected_content)\n",
    "\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "    \n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "    \n",
    "                    # Use current date and time in IST\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "                    #yesterday_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\")) - timedelta(days=1)\n",
    "    \n",
    "    \n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"user123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin_02\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "    \n",
    "                    # Specify the MongoDB database and collection names\n",
    "                    database_name = \"NewsAnalytics\"\n",
    "                    collection_name = \"RawNews_Hindu\"\n",
    "                    # Insert the record into MongoDB\n",
    "                    insert_record_into_mongodb(record, database_name, collection_name)\n",
    "        print(\"Completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the HTML: {e}\")\n",
    "\n",
    "def economictimes_headlines():\n",
    "    print(\"Fetching headlines from Economic Times...\")\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-03-january-2024/article67698291.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-02-january-2024/article67695347.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-markets/share-market-nifty-sensex-live-updates-04-january-2024/article67702215.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-05-january-2024/article67706161.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-08-january-2024/article67715949.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-09-january-2024/article67720535.ece\"\n",
    "    #url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-10-january-2024/article67724321.ece\"\n",
    "    ## url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-11-january-2024/article67727041.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-12-january-2024/article67730865.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-15-january-2024/article67740526.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-16-january-2024/article67743130.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-17-january-2023/article67744658.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-18-january-2024/article67748508.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-19-january-2024/article67752291.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-20-january-2024/article67757092.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/stock-market-highlights-23-january-2024/article67762366.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-24-january-2024/article67768511.ece\"\n",
    "    # url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-updates-25-january-2024/article67773490.ece\"\n",
    "    url = \"https://www.thehindubusinessline.com/markets/share-market-nifty-sensex-live-update-29-january-2024/article67786136.ece\"\n",
    "\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        page_request = requests.get(url)\n",
    "        page_request.raise_for_status()\n",
    "        print(\"Status:\", page_request.status_code)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making the request: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = page_request.content\n",
    "        sentiment_keywords = load_sentiment_keywords()\n",
    "        read_html_data(data, sentiment_keywords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing the page: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    economictimes_headlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1304596-05e6-430c-b420-b2bca4e5222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "update existing record with FINBERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d93636-e980-461c-a9c0-0bdd5a744441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record 65b7cc2e9cd55a47d7c80a35 - FinBertScore: 0, Updated at: 2024-01-29 21:35:03.792922+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc309cd55a47d7c80a37 - FinBertScore: 0, Updated at: 2024-01-29 21:35:03.938462+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc329cd55a47d7c80a39 - FinBertScore: 0, Updated at: 2024-01-29 21:35:04.081526+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc349cd55a47d7c80a3b - FinBertScore: 0, Updated at: 2024-01-29 21:35:04.217962+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc359cd55a47d7c80a3d - FinBertScore: 1, Updated at: 2024-01-29 21:35:04.430573+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc369cd55a47d7c80a3f - FinBertScore: 0, Updated at: 2024-01-29 21:35:04.596267+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc379cd55a47d7c80a41 - FinBertScore: 0, Updated at: 2024-01-29 21:35:04.915548+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc389cd55a47d7c80a43 - FinBertScore: 0, Updated at: 2024-01-29 21:35:05.118402+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc3a9cd55a47d7c80a46 - FinBertScore: 0, Updated at: 2024-01-29 21:35:05.449603+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc3b9cd55a47d7c80a48 - FinBertScore: 0, Updated at: 2024-01-29 21:35:05.748372+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc3c9cd55a47d7c80a4a - FinBertScore: 0, Updated at: 2024-01-29 21:35:06.048536+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc3e9cd55a47d7c80a4c - FinBertScore: 0, Updated at: 2024-01-29 21:35:06.306186+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc419cd55a47d7c80a4e - FinBertScore: 0, Updated at: 2024-01-29 21:35:06.673413+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc439cd55a47d7c80a50 - FinBertScore: 0, Updated at: 2024-01-29 21:35:06.946784+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc449cd55a47d7c80a52 - FinBertScore: 1, Updated at: 2024-01-29 21:35:07.261564+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc469cd55a47d7c80a54 - FinBertScore: 0, Updated at: 2024-01-29 21:35:07.533279+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc489cd55a47d7c80a56 - FinBertScore: 0, Updated at: 2024-01-29 21:35:07.787727+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc499cd55a47d7c80a58 - FinBertScore: 0, Updated at: 2024-01-29 21:35:08.039850+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc4b9cd55a47d7c80a5a - FinBertScore: 0, Updated at: 2024-01-29 21:35:08.228030+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc4c9cd55a47d7c80a5c - FinBertScore: 0, Updated at: 2024-01-29 21:35:08.492400+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc4d9cd55a47d7c80a5e - FinBertScore: 0, Updated at: 2024-01-29 21:35:08.768537+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc4f9cd55a47d7c80a60 - FinBertScore: 0, Updated at: 2024-01-29 21:35:09.044050+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc509cd55a47d7c80a62 - FinBertScore: 0, Updated at: 2024-01-29 21:35:09.307730+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc529cd55a47d7c80a64 - FinBertScore: 2, Updated at: 2024-01-29 21:35:09.638237+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc539cd55a47d7c80a66 - FinBertScore: 0, Updated at: 2024-01-29 21:35:09.894864+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc549cd55a47d7c80a68 - FinBertScore: 0, Updated at: 2024-01-29 21:35:10.106986+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc569cd55a47d7c80a6a - FinBertScore: 0, Updated at: 2024-01-29 21:35:10.324712+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc579cd55a47d7c80a6c - FinBertScore: 0, Updated at: 2024-01-29 21:35:10.542568+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc599cd55a47d7c80a6e - FinBertScore: 0, Updated at: 2024-01-29 21:35:10.762129+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc5a9cd55a47d7c80a70 - FinBertScore: 0, Updated at: 2024-01-29 21:35:11.031026+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc5b9cd55a47d7c80a72 - FinBertScore: 0, Updated at: 2024-01-29 21:35:11.285887+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc5c9cd55a47d7c80a74 - FinBertScore: 0, Updated at: 2024-01-29 21:35:11.538726+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc5d9cd55a47d7c80a76 - FinBertScore: 0, Updated at: 2024-01-29 21:35:11.764232+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc5f9cd55a47d7c80a78 - FinBertScore: 0, Updated at: 2024-01-29 21:35:11.974294+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc609cd55a47d7c80a7a - FinBertScore: 0, Updated at: 2024-01-29 21:35:12.229321+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc629cd55a47d7c80a7c - FinBertScore: 0, Updated at: 2024-01-29 21:35:12.495001+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc639cd55a47d7c80a7e - FinBertScore: 0, Updated at: 2024-01-29 21:35:12.701241+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc649cd55a47d7c80a80 - FinBertScore: 0, Updated at: 2024-01-29 21:35:12.943691+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc669cd55a47d7c80a82 - FinBertScore: 0, Updated at: 2024-01-29 21:35:13.190390+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc679cd55a47d7c80a84 - FinBertScore: 0, Updated at: 2024-01-29 21:35:13.394177+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc699cd55a47d7c80a86 - FinBertScore: 0, Updated at: 2024-01-29 21:35:13.658464+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc6a9cd55a47d7c80a88 - FinBertScore: 0, Updated at: 2024-01-29 21:35:13.873974+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc6b9cd55a47d7c80a8a - FinBertScore: 0, Updated at: 2024-01-29 21:35:14.229401+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc6d9cd55a47d7c80a8c - FinBertScore: 0, Updated at: 2024-01-29 21:35:14.515676+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc6e9cd55a47d7c80a8e - FinBertScore: 0, Updated at: 2024-01-29 21:35:14.853381+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc6f9cd55a47d7c80a90 - FinBertScore: 0, Updated at: 2024-01-29 21:35:15.306495+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc709cd55a47d7c80a92 - FinBertScore: 0, Updated at: 2024-01-29 21:35:15.625250+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc729cd55a47d7c80a94 - FinBertScore: 0, Updated at: 2024-01-29 21:35:16.122073+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc739cd55a47d7c80a96 - FinBertScore: 0, Updated at: 2024-01-29 21:35:16.499382+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc749cd55a47d7c80a98 - FinBertScore: 0, Updated at: 2024-01-29 21:35:16.824437+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc759cd55a47d7c80a9a - FinBertScore: 0, Updated at: 2024-01-29 21:35:17.179146+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc779cd55a47d7c80a9c - FinBertScore: 0, Updated at: 2024-01-29 21:35:17.510374+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc789cd55a47d7c80a9e - FinBertScore: 0, Updated at: 2024-01-29 21:35:17.821868+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc799cd55a47d7c80aa0 - FinBertScore: 0, Updated at: 2024-01-29 21:35:18.157057+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc7a9cd55a47d7c80aa2 - FinBertScore: 0, Updated at: 2024-01-29 21:35:18.478303+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc7b9cd55a47d7c80aa4 - FinBertScore: 0, Updated at: 2024-01-29 21:35:18.817960+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc7d9cd55a47d7c80aa6 - FinBertScore: 0, Updated at: 2024-01-29 21:35:19.109714+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65b7cc7e9cd55a47d7c80aa8 - FinBertScore: 0, Updated at: 2024-01-29 21:35:19.432433+05:30, Updated by: FIN_BERT_Admin\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "# MongoDB connection URI (replace with your actual URI)\n",
    "uri = \"mongodb://localhost:27017\"\n",
    "# uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# def analyze_finBERT_sentiment(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "#     outputs = finbert(**inputs)[0]\n",
    "#     sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "#     return sentiment_label\n",
    "\n",
    "def analyze_finBERT_sentiment(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        outputs = finbert(**inputs)[0]\n",
    "        sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "        return sentiment_label\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing FinBERT sentiment: {e}\")\n",
    "        return 0\n",
    "\n",
    "try:\n",
    "    with MongoClient(uri) as client:\n",
    "        # Specify the database and collection\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_StockEdge\"\n",
    "        # Access the specified collection\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        # Query all records in the collection\n",
    "        all_records = collection.find()\n",
    "\n",
    "        # Get the total count of records\n",
    "        total_records = collection.count_documents({})\n",
    "\n",
    "        # Iterate over each record\n",
    "        for record in all_records:\n",
    "            if \"paragraph_content\" in record and \"FinBertScore\" not in record:\n",
    "                paragraph_content = record[\"paragraph_content\"]\n",
    "\n",
    "                # Analyze FinBERT sentiment for the paragraph_content\n",
    "                sentiment_label = analyze_finBERT_sentiment(paragraph_content)\n",
    "                # Format the date in the desired format\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                # Update the record with the new field \"FinBertScore\" and metadata\n",
    "                update_data = {\n",
    "                    \"$set\": {\n",
    "                        \"FinBertScore\": int(sentiment_label),\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"FIN_BERT_Admin\"\n",
    "                    }\n",
    "                }\n",
    "                collection.update_one({\"_id\": record[\"_id\"]}, update_data)\n",
    "\n",
    "                # Print statement for successful entry\n",
    "                print(f\"Processed record {record['_id']} - FinBertScore: {int(sentiment_label)}, Updated at: {current_datetime_ist}, Updated by: FIN_BERT_Admin\")\n",
    "        print(\"Completed\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d89d50-5015-45d1-88af-53cad8041d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock Edge UI path ALL news insert | excel read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974433a0-ebb0-4ddb-be54-e9ab1f8f3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip23012024.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "        return set(df.iloc[:, 0].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    try:\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, database_name, collection_name):\n",
    "    uri = \"mongodb://localhost:27017\"\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    collection = client[database_name][collection_name]\n",
    "\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    # Check if the paragraph_content already exists in the collection\n",
    "    existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "    return existing_record is not None\n",
    "\n",
    "def read_excel_data(excel_path, sheet_name, sentiment_keywords):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            paragraph_content = row[0]  # Access content from the first column\n",
    "            sentiment_result = analyze_sentiment(paragraph_content)\n",
    "\n",
    "            if sentiment_result:\n",
    "                sentiment_data = {\n",
    "                    \"label\": sentiment_result[\"label\"],\n",
    "                    \"confidence\": sentiment_result[\"score\"]\n",
    "                }\n",
    "\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                record = {\n",
    "                    \"paragraph_content\": paragraph_content,\n",
    "                    \"sentiment\": sentiment_data,\n",
    "                    \"created_at\": current_datetime_ist,\n",
    "                    \"created_by\": \"StockEdge123\",\n",
    "                    \"deleted_at\": current_datetime_ist,\n",
    "                    \"deleted_by\": \"user789\",\n",
    "                    \"is_deleted\": False,\n",
    "                    \"is_purged\": False,\n",
    "                    \"last_scraped\": current_datetime_ist,\n",
    "                    \"purge_at\": current_datetime_ist,\n",
    "                    \"purged_by\": \"admin\",\n",
    "                    \"updated_at\": current_datetime_ist,\n",
    "                    \"updated_by\": \"user456\"\n",
    "                }\n",
    "\n",
    "                database_name = \"NewsAnalytics\"\n",
    "                collection_name = \"RawNews_StockEdge\"\n",
    "                \n",
    "                insert_record_into_mongodb(record, database_name, collection_name)\n",
    "        print(\"Completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\news\\stockedge\\scrapestockedge.xlsx\"\n",
    "    sheet_name = \"Sheet1\"\n",
    "    # sentiment_keywords = load_sentiment_keywords()\n",
    "    read_excel_data(excel_path, sheet_name, sentiment_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b9e01-dfd8-46c7-8971-e2c6f57e08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock Edge UI path excel read only for Sentiment keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec504ab-6912-43eb-8be4-466c44d4e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip090120241753.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "        return set(df.iloc[:, 0].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, collection):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists in the collection\n",
    "        existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "        return existing_record is not None\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking existing record in MongoDB: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_excel_data(excel_path, sheet_name, sentiment_keywords):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "        \n",
    "        # Connect to MongoDB\n",
    "        uri = \"mongodb://localhost:27017\"\n",
    "        client = MongoClient(uri)\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_StockEdge\"\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            paragraph_content = row[0]  # Access content from the first column\n",
    "\n",
    "            # Check if the paragraph_content contains any of the keywords\n",
    "            if any(keyword in paragraph_content.lower() for keyword in sentiment_keywords):\n",
    "                sentiment_result = analyze_sentiment(paragraph_content)\n",
    "\n",
    "                if sentiment_result:\n",
    "                    sentiment_data = {\n",
    "                        \"label\": sentiment_result[\"label\"],\n",
    "                        \"confidence\": sentiment_result[\"score\"]\n",
    "                    }\n",
    "\n",
    "                    current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                    record = {\n",
    "                        \"paragraph_content\": paragraph_content,\n",
    "                        \"sentiment\": sentiment_data,\n",
    "                        \"created_at\": current_datetime_ist,\n",
    "                        \"created_by\": \"StockEdge123\",\n",
    "                        \"deleted_at\": current_datetime_ist,\n",
    "                        \"deleted_by\": \"user789\",\n",
    "                        \"is_deleted\": False,\n",
    "                        \"is_purged\": False,\n",
    "                        \"last_scraped\": current_datetime_ist,\n",
    "                        \"purge_at\": current_datetime_ist,\n",
    "                        \"purged_by\": \"admin\",\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"user456\"\n",
    "                    }\n",
    "                    insert_record_into_mongodb(record, collection)\n",
    "\n",
    "        print(\"Completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel data: {e}\")\n",
    "    finally:\n",
    "        # Close the MongoDB connection\n",
    "        client.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\news\\stockedge\\scrapestockedge.xlsx\"\n",
    "    sheet_name = \"Sheet1\"\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    read_excel_data(excel_path, sheet_name, sentiment_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41f372-5f4a-494f-b51e-a45c64a255dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
