{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ec3581af-331b-4ebd-a6d1-764500caa916",
   "metadata": {},
   "source": [
    "Stock Edge UI path ALL news insert | excel read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8b4d34-9d35-4ef2-81e8-c7e75b219f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91908\\AppData\\Local\\Temp\\ipykernel_16652\\3097768751.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record inserted successfully.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Record with paragraph_content already exists. Skipping insertion.\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_sentiment_keywords():\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip23012024.xlsx\"\n",
    "    sheet_name = \"Sentiment\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "        return set(df.iloc[:, 0].str.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentiment keywords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    try:\n",
    "        result = sentiment_analysis(text)\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_record_into_mongodb(record, database_name, collection_name):\n",
    "    uri = \"mongodb://localhost:27017\"\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    collection = client[database_name][collection_name]\n",
    "\n",
    "    try:\n",
    "        # Check if the paragraph_content already exists\n",
    "        if not record_exists_in_mongodb(collection, record[\"paragraph_content\"]):\n",
    "            # Insert the record into the collection\n",
    "            collection.insert_one(record)\n",
    "            print(\"Record inserted successfully.\")\n",
    "        else:\n",
    "            print(\"Record with paragraph_content already exists. Skipping insertion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record into MongoDB: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def record_exists_in_mongodb(collection, paragraph_content):\n",
    "    # Check if the paragraph_content already exists in the collection\n",
    "    existing_record = collection.find_one({\"paragraph_content\": paragraph_content})\n",
    "    return existing_record is not None\n",
    "\n",
    "def read_excel_data(excel_path, sheet_name, sentiment_keywords):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None, skiprows=1)\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            paragraph_content = row[0]  # Access content from the first column\n",
    "            sentiment_result = analyze_sentiment(paragraph_content)\n",
    "\n",
    "            if sentiment_result:\n",
    "                sentiment_data = {\n",
    "                    \"label\": sentiment_result[\"label\"],\n",
    "                    \"confidence\": sentiment_result[\"score\"]\n",
    "                }\n",
    "\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                record = {\n",
    "                    \"paragraph_content\": paragraph_content,\n",
    "                    \"sentiment\": sentiment_data,\n",
    "                    \"created_at\": current_datetime_ist,\n",
    "                    \"created_by\": \"StockEdge123\",\n",
    "                    \"deleted_at\": current_datetime_ist,\n",
    "                    \"deleted_by\": \"user789\",\n",
    "                    \"is_deleted\": False,\n",
    "                    \"is_purged\": False,\n",
    "                    \"last_scraped\": current_datetime_ist,\n",
    "                    \"purge_at\": current_datetime_ist,\n",
    "                    \"purged_by\": \"admin\",\n",
    "                    \"updated_at\": current_datetime_ist,\n",
    "                    \"updated_by\": \"user456\"\n",
    "                }\n",
    "\n",
    "                database_name = \"NewsAnalytics\"\n",
    "                collection_name = \"RawNews_StockEdge\"\n",
    "                \n",
    "                insert_record_into_mongodb(record, database_name, collection_name)\n",
    "        print(\"Completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = r\"C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\news\\stockedge\\scrapestockedge.xlsx\"\n",
    "    sheet_name = \"Sheet1\"\n",
    "    sentiment_keywords = load_sentiment_keywords()\n",
    "    read_excel_data(excel_path, sheet_name, sentiment_keywords)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cbd05e7e-3eb5-4630-8817-f0516fb81f75",
   "metadata": {},
   "source": [
    "update existing record with FINBERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27ffcd9-1717-4083-8746-72483905ba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record 65c39eabc62b12b7cc360848 - FinBertScore: 0, Updated at: 2024-02-07 20:48:30.729981+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39eadc62b12b7cc36084a - FinBertScore: 0, Updated at: 2024-02-07 20:48:30.862049+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39eb0c62b12b7cc36084c - FinBertScore: 0, Updated at: 2024-02-07 20:48:31.027614+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39eb2c62b12b7cc36084e - FinBertScore: 0, Updated at: 2024-02-07 20:48:31.177749+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39eb5c62b12b7cc360850 - FinBertScore: 2, Updated at: 2024-02-07 20:48:31.328514+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39eb6c62b12b7cc360852 - FinBertScore: 0, Updated at: 2024-02-07 20:48:31.494571+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39eb8c62b12b7cc360854 - FinBertScore: 0, Updated at: 2024-02-07 20:48:31.643607+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39eb9c62b12b7cc360856 - FinBertScore: 0, Updated at: 2024-02-07 20:48:31.835443+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ebbc62b12b7cc360858 - FinBertScore: 0, Updated at: 2024-02-07 20:48:32.024500+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ebcc62b12b7cc36085a - FinBertScore: 0, Updated at: 2024-02-07 20:48:32.221565+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ebdc62b12b7cc36085c - FinBertScore: 0, Updated at: 2024-02-07 20:48:32.407183+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ebfc62b12b7cc36085e - FinBertScore: 0, Updated at: 2024-02-07 20:48:32.587297+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ec0c62b12b7cc360860 - FinBertScore: 1, Updated at: 2024-02-07 20:48:32.799415+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ec2c62b12b7cc360862 - FinBertScore: 1, Updated at: 2024-02-07 20:48:33.042518+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ec4c62b12b7cc360864 - FinBertScore: 0, Updated at: 2024-02-07 20:48:33.247788+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ec5c62b12b7cc360866 - FinBertScore: 1, Updated at: 2024-02-07 20:48:33.442873+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ec7c62b12b7cc360868 - FinBertScore: 0, Updated at: 2024-02-07 20:48:33.625943+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ec9c62b12b7cc36086a - FinBertScore: 0, Updated at: 2024-02-07 20:48:33.798045+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ecbc62b12b7cc36086c - FinBertScore: 0, Updated at: 2024-02-07 20:48:34.009839+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ecec62b12b7cc36086e - FinBertScore: 0, Updated at: 2024-02-07 20:48:34.276497+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ed0c62b12b7cc360870 - FinBertScore: 0, Updated at: 2024-02-07 20:48:34.474601+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ed1c62b12b7cc360872 - FinBertScore: 0, Updated at: 2024-02-07 20:48:34.685431+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ed2c62b12b7cc360874 - FinBertScore: 0, Updated at: 2024-02-07 20:48:34.893458+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ed3c62b12b7cc360876 - FinBertScore: 0, Updated at: 2024-02-07 20:48:35.093535+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ed4c62b12b7cc360878 - FinBertScore: 0, Updated at: 2024-02-07 20:48:35.296636+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ed6c62b12b7cc36087a - FinBertScore: 0, Updated at: 2024-02-07 20:48:35.496865+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ed7c62b12b7cc36087c - FinBertScore: 0, Updated at: 2024-02-07 20:48:35.714956+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ed8c62b12b7cc36087e - FinBertScore: 0, Updated at: 2024-02-07 20:48:35.909061+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39edac62b12b7cc360880 - FinBertScore: 0, Updated at: 2024-02-07 20:48:36.127169+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39edbc62b12b7cc360882 - FinBertScore: 0, Updated at: 2024-02-07 20:48:36.334309+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39edcc62b12b7cc360884 - FinBertScore: 0, Updated at: 2024-02-07 20:48:36.506437+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39edec62b12b7cc360886 - FinBertScore: 0, Updated at: 2024-02-07 20:48:36.712522+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39edfc62b12b7cc360888 - FinBertScore: 0, Updated at: 2024-02-07 20:48:36.916635+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ee1c62b12b7cc36088a - FinBertScore: 0, Updated at: 2024-02-07 20:48:37.131773+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ee2c62b12b7cc36088c - FinBertScore: 0, Updated at: 2024-02-07 20:48:37.336860+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ee3c62b12b7cc36088e - FinBertScore: 0, Updated at: 2024-02-07 20:48:37.592553+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ee5c62b12b7cc360890 - FinBertScore: 0, Updated at: 2024-02-07 20:48:37.807890+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ee6c62b12b7cc360892 - FinBertScore: 0, Updated at: 2024-02-07 20:48:38.013009+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ee7c62b12b7cc360894 - FinBertScore: 0, Updated at: 2024-02-07 20:48:38.280752+05:30, Updated by: FIN_BERT_Admin\n",
      "Processed record 65c39ee9c62b12b7cc360896 - FinBertScore: 0, Updated at: 2024-02-07 20:48:38.582373+05:30, Updated by: FIN_BERT_Admin\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "# MongoDB connection URI (replace with your actual URI)\n",
    "uri = \"mongodb://localhost:27017\"\n",
    "# uri = \"mongodb+srv://ranguchamy:J8ePGYKw7XRdYZBg@stockanalytics.jkcqv2m.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# def analyze_finBERT_sentiment(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "#     outputs = finbert(**inputs)[0]\n",
    "#     sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "#     return sentiment_label\n",
    "\n",
    "def analyze_finBERT_sentiment(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        outputs = finbert(**inputs)[0]\n",
    "        sentiment_label = np.argmax(outputs.detach().numpy())\n",
    "        return sentiment_label\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing FinBERT sentiment: {e}\")\n",
    "        return 0\n",
    "\n",
    "try:\n",
    "    with MongoClient(uri) as client:\n",
    "        # Specify the database and collection\n",
    "        database_name = \"NewsAnalytics\"\n",
    "        collection_name = \"RawNews_StockEdge\"\n",
    "        # Access the specified collection\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        # Query all records in the collection\n",
    "        all_records = collection.find()\n",
    "\n",
    "        # Get the total count of records\n",
    "        total_records = collection.count_documents({})\n",
    "\n",
    "        # Iterate over each record\n",
    "        for record in all_records:\n",
    "            if \"paragraph_content\" in record and \"FinBertScore\" not in record:\n",
    "                paragraph_content = record[\"paragraph_content\"]\n",
    "\n",
    "                # Analyze FinBERT sentiment for the paragraph_content\n",
    "                sentiment_label = analyze_finBERT_sentiment(paragraph_content)\n",
    "                # Format the date in the desired format\n",
    "                current_datetime_ist = datetime.now(pytz.timezone(\"Asia/Kolkata\"))\n",
    "\n",
    "                # Update the record with the new field \"FinBertScore\" and metadata\n",
    "                update_data = {\n",
    "                    \"$set\": {\n",
    "                        \"FinBertScore\": int(sentiment_label),\n",
    "                        \"updated_at\": current_datetime_ist,\n",
    "                        \"updated_by\": \"FIN_BERT_Admin\"\n",
    "                    }\n",
    "                }\n",
    "                collection.update_one({\"_id\": record[\"_id\"]}, update_data)\n",
    "\n",
    "                # Print statement for successful entry\n",
    "                print(f\"Processed record {record['_id']} - FinBertScore: {int(sentiment_label)}, Updated at: {current_datetime_ist}, Updated by: FIN_BERT_Admin\")\n",
    "        print(\"Completed\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d30916-5b28-49d6-ab37-ec195055e789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
