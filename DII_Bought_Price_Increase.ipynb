{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba6a559-a885-4b86-ac45-d68ea63e7a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "2023-10-05 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\Users\\91908\\AppData\\Roaming\\Python\\Python39\\site-packages\\yfinance\\utils.py:782: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "Percentage Changes:\n",
      "+------------+----------------------------------------------+----------------+-----------------+---------------+---------------------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "| Stock Code |                 Client Name                  |    Holding     | Percent Holding | Investor Type |      File Date      | 1D P (%) | 1D V (%) | 20D P (%) | 20D V (%) | 40D P (%) | 40D V (%) | 60D P (%) | 60D V (%) |\n",
      "+------------+----------------------------------------------+----------------+-----------------+---------------+---------------------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|   AFFLE    |                 ICICI Group                  | Sold -0.22000  |       6.4       | Institutional | 2023-10-05 00:00:00 |   0.19   |  144.51  |   -2.57   |   -9.65   |    3.22   |   130.65  |   20.04   |   232.94  |\n",
      "|  HDFCBANK  |                 ICICI Group                  | Bought 0.48000 |       1.84      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |\n",
      "|  MOLDTECH  |                Uno Metals Ltd                | Sold -0.01000  |       3.8       | Institutional | 2023-10-05 00:00:00 |   1.85   |  12.80   |   -25.29  |  1147.25  |   -30.96  |   528.40  |   -20.80  |   134.37  |\n",
      "|   AETHER   |            Sbi Magnum Midcap Fund            | Sold -0.02000  |       8.78      | Institutional | 2023-10-05 00:00:00 |  -0.29   |  -83.69  |   -7.60   |   -70.73  |   -18.99  |   124.49  |   -6.47   |   131.36  |\n",
      "| TORNTPOWER |          Axis Long Term Equity Fund          | Sold -0.61000  |       8.05      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |\n",
      "|  HDFCBANK  |   Icici Prudential Balanced Advantage Fund   | Bought 0.48000 |       1.84      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |\n",
      "|  HDFCBANK  |              Hdfc Balanced Fund              | Bought 0.26000 |       2.12      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |\n",
      "| MOLDTKPAC  |                SUNDARAM-GROUP                | Bought 0.07000 |       2.91      | Institutional | 2023-10-05 00:00:00 |  -0.73   |  33.62   |    0.28   |   245.77  |   -2.29   |   -23.22  |    3.69   |   67.93   |\n",
      "| TORNTPOWER | Hdfc Standard Life Insurance Company Limited | Bought 0.12000 |       1.26      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |\n",
      "|   KRSNAA   | Hdfc Standard Life Insurance Company Limited | Bought 0.16000 |       1.98      | Institutional | 2023-10-05 00:00:00 |   0.94   |  -6.41   |    5.93   |   -1.68   |   -1.33   |   -27.89  |    4.00   |   63.12   |\n",
      "|  ROSSARI   |                 CANARA-GROUP                 | Bought 0.31000 |       1.93      | Institutional | 2023-10-05 00:00:00 |   1.13   |  229.35  |   -13.98  |   204.50  |   -4.12   |  2449.79  |   -4.94   |   102.65  |\n",
      "| MOLDTKPAC  |                 CANARA-GROUP                 | Bought 0.05000 |       2.57      | Institutional | 2023-10-05 00:00:00 |  -0.73   |  33.62   |    0.28   |   245.77  |   -2.29   |   -23.22  |    3.69   |   67.93   |\n",
      "|   AFFLE    | Icici Prudential Life Insurance Company Ltd  | Sold -0.13000  |       3.07      | Institutional | 2023-10-05 00:00:00 |   0.19   |  144.51  |   -2.57   |   -9.65   |    3.22   |   130.65  |   20.04   |   232.94  |\n",
      "|  HDFCBANK  |         Hdfc Trustee Company Limited         | Bought 0.26000 |       2.12      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |\n",
      "| TORNTPOWER |                  Axis Group                  | Sold -0.61000  |       8.05      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |\n",
      "|   AETHER   |                  Axis Group                  | Sold -0.13000  |       1.26      | Institutional | 2023-10-05 00:00:00 |  -0.29   |  -83.69  |   -7.60   |   -70.73  |   -18.99  |   124.49  |   -6.47   |   131.36  |\n",
      "|   KRSNAA   |                 Birla Group                  | Sold -1.94000  |       2.05      | Institutional | 2023-10-05 00:00:00 |   0.94   |  -6.41   |    5.93   |   -1.68   |   -1.33   |   -27.89  |    4.00   |   63.12   |\n",
      "| TORNTPOWER |                  SBI Group                   | Bought 0.26000 |       5.47      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |\n",
      "|  ROSSARI   |                  SBI Group                   | Sold -0.02000  |       6.66      | Institutional | 2023-10-05 00:00:00 |   1.13   |  229.35  |   -13.98  |   204.50  |   -4.12   |  2449.79  |   -4.94   |   102.65  |\n",
      "|   AETHER   |                  SBI Group                   | Sold -0.02000  |       8.78      | Institutional | 2023-10-05 00:00:00 |  -0.29   |  -83.69  |   -7.60   |   -70.73  |   -18.99  |   124.49  |   -6.47   |   131.36  |\n",
      "| TORNTPOWER |                  HDFC Group                  | Bought 0.12000 |       1.26      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |\n",
      "|   KRSNAA   |                  HDFC Group                  | Bought 0.16000 |       1.98      | Institutional | 2023-10-05 00:00:00 |   0.94   |  -6.41   |    5.93   |   -1.68   |   -1.33   |   -27.89  |    4.00   |   63.12   |\n",
      "|  HDFCBANK  |                  HDFC Group                  | Bought 0.26000 |       2.12      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |\n",
      "| MOLDTKPAC  |                 ICICI Group                  | Bought 0.08000 |       3.41      | Institutional | 2023-10-05 00:00:00 |  -0.73   |  33.62   |    0.28   |   245.77  |   -2.29   |   -23.22  |    3.69   |   67.93   |\n",
      "|   KRSNAA   |                 ICICI Group                  | Bought 0.10000 |       5.4       | Institutional | 2023-10-05 00:00:00 |   0.94   |  -6.41   |    5.93   |   -1.68   |   -1.33   |   -27.89  |    4.00   |   63.12   |\n",
      "|  MOLDTECH  |               Akg Finvest Ltd                | Sold -0.05000  |       3.75      | Institutional | 2023-10-05 00:00:00 |   1.85   |  12.80   |   -25.29  |  1147.25  |   -30.96  |   528.40  |   -20.80  |   134.37  |\n",
      "+------------+----------------------------------------------+----------------+-----------------+---------------+---------------------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from prettytable import PrettyTable\n",
    "import os\n",
    "from pandas_market_calendars import get_calendar\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "# Define the path to the CSV file containing 'Client Name'\n",
    "csv_path = r'C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Institutional_Investor_Data\\Individual_Investor_LIST_240130164809.csv'\n",
    "\n",
    "# Read 'Client Name' from the CSV file\n",
    "client_name_df = pd.read_csv(csv_path)\n",
    "client_names = client_name_df['Client Name'].tolist()\n",
    "\n",
    "# Define the path to the folder containing Excel files\n",
    "excel_folder_path = r'C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\OutboundSuperstars'\n",
    "\n",
    "# Define the start and end dates for the file date range\n",
    "start_date = datetime.strptime('20231001', \"%Y%m%d\")\n",
    "end_date = datetime.strptime('20231005', \"%Y%m%d\")\n",
    "\n",
    "# Create a PrettyTable to store the results\n",
    "results_table = PrettyTable()\n",
    "results_table.field_names = ['Code', 'Client Name', 'Holding', 'Percent Holding', 'Investor Type', 'File Date']\n",
    "\n",
    "# Create a list to store matching rows\n",
    "matching_rows = []\n",
    "\n",
    "# Iterate through each Excel file\n",
    "for excel_file in os.listdir(excel_folder_path):\n",
    "    excel_file_path = os.path.join(excel_folder_path, excel_file)\n",
    "\n",
    "    # Get the list of sheet names in the Excel file\n",
    "    sheet_names = pd.ExcelFile(excel_file_path).sheet_names\n",
    "\n",
    "    # Check if 'Shareholding' sheet exists\n",
    "    if 'Shareholding' in sheet_names:\n",
    "        # Read the 'Shareholding' sheet\n",
    "        df = pd.read_excel(excel_file_path, sheet_name='Shareholding', skiprows=6)\n",
    "\n",
    "        # Extract date from the file name using regular expression\n",
    "        match = re.search(r'\\d{8}', excel_file)  # Assuming the date in the file name is in YYYYMMDD format\n",
    "        if match:\n",
    "            file_date_str = match.group(0)\n",
    "            file_date = datetime.strptime(file_date_str, \"%Y%m%d\")\n",
    "\n",
    "            # Check if the file date is within the specified range\n",
    "            if start_date <= file_date <= end_date:\n",
    "                # Check if 'Client Name' is present in the DataFrame\n",
    "                if 'Client Name' in df.columns:\n",
    "                    # Check if each 'Client Name' from the CSV is present in the Excel file\n",
    "                    present_in_excel = [client_name in df['Client Name'].values for client_name in client_names]\n",
    "\n",
    "                    # Display only the specified columns for matched records\n",
    "                    for i, present in enumerate(present_in_excel):\n",
    "                        if present:\n",
    "                            matching_row = df[df['Client Name'] == client_names[i]]\n",
    "                            matching_row = matching_row[['Code', 'Client Name', 'Holding', 'Percent Holding', 'Investor Type']]\n",
    "                            matching_row['File Date'] = file_date_str\n",
    "                            matching_rows.append(matching_row)\n",
    "\n",
    "# Concatenate all matching rows into a DataFrame\n",
    "results_df = pd.concat(matching_rows)\n",
    "\n",
    "# Sort the DataFrame by 'File Date' in descending order\n",
    "results_df = results_df.sort_values(by='File Date', ascending=False)\n",
    "\n",
    "# Print the results using PrettyTable\n",
    "for _, row in results_df.iterrows():\n",
    "    results_table.add_row(row.tolist())\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def is_working_day(date_str):\n",
    "    # Parse the date string to datetime format\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    \n",
    "    # Check if the date falls on a weekend (Saturday or Sunday)\n",
    "    if date_obj.weekday() >= 5:\n",
    "        return False\n",
    "    \n",
    "    # Check if the date falls on a special working day\n",
    "    special_working_days = {\"2024-01-27\"}  # Example: Christmas Eve\n",
    "    if date_str in special_working_days:\n",
    "        return True\n",
    "    \n",
    "    # Check if the date falls on a holiday\n",
    "    # Add your holiday logic here\n",
    "    holidays = {\"2023-01-26\", \"2023-03-07\", \"2023-03-30\", \"2023-04-04\", \"2023-04-07\", \n",
    "                \"2023-04-14\", \"2023-05-01\", \"2023-06-29\", \"2023-07-29\", \"2023-08-15\", \n",
    "                \"2023-09-19\", \"2023-10-02\", \"2023-10-24\", \"2023-11-14\", \"2023-11-27\", \n",
    "                \"2023-12-25\",\"2024-01-22\",\n",
    "                \"2024-01-26\", \"2024-03-08\", \"2024-03-25\", \"2024-03-29\", \"2024-04-11\", \n",
    "                \"2024-04-17\", \"2024-05-01\", \"2024-06-17\", \"2024-07-17\", \"2024-08-15\", \n",
    "                \"2024-10-02\", \"2024-11-01\", \"2024-11-15\", \"2024-12-25\"}\n",
    "    if date_str in holidays:\n",
    "        return False\n",
    "    \n",
    "    # If the date is not a weekend, holiday, or special working day, it's a regular working day\n",
    "    return True\n",
    "\n",
    "def is_working_day_if_not_next_working_date(date):\n",
    "    # Check if the input is already a string\n",
    "    if isinstance(date, str):\n",
    "        # Parse the date string to datetime format\n",
    "        date_obj = datetime.strptime(date, '%Y-%m-%d')\n",
    "    else:\n",
    "        # Use the provided Timestamp object\n",
    "        date_obj = date\n",
    "    \n",
    "    # Check if the date is a working day\n",
    "    if is_working_day(date_obj.strftime('%Y-%m-%d')):\n",
    "        return date_obj\n",
    "    \n",
    "    # If the date is not a working day, find the next working date\n",
    "    while True:\n",
    "        date_obj += timedelta(days=1)\n",
    "        if is_working_day(date_obj.strftime('%Y-%m-%d')):\n",
    "            return date_obj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_stock_data(symbol, start_date, end_date):\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "def calculate_percentage_changes(data, stock_code, start_date,client_name,holding,percent_holding,investor_type,file_date ):\n",
    "    result_dict = {\n",
    "        \"Stock Code\": stock_code,\n",
    "        \"Client Name\": client_name,  # Corrected variable name\n",
    "        \"Holding\": holding,  # Corrected variable name\n",
    "        \"Percent Holding\": percent_holding,  # Corrected variable name\n",
    "        \"Investor Type\": investor_type,  # Corrected variable name\n",
    "        \"File Date\": file_date,  # Corrected variable name\n",
    "    }\n",
    "\n",
    "\n",
    "    periods = [1, 20, 40, 60]\n",
    "\n",
    "    for period in periods:\n",
    "         if len(data) >= period + 1:\n",
    "\n",
    "            close_change_1d = volume_change = None\n",
    "    \n",
    "            # Check if the start date is in the index\n",
    "            if start_date in data.index:\n",
    "                close_change_1d = ((data['Close'].iloc[period] - data['Close'].loc[start_date]) / data['Close'].loc[start_date]) * 100\n",
    "                volume_change = ((data['Volume'].iloc[period] - data['Volume'].loc[start_date]) / data['Volume'].loc[start_date]) * 100\n",
    "    \n",
    "            result_dict[f\"{period}D P (%)\"] = f\"{close_change_1d:.2f}\" if close_change_1d is not None else \"nan\"\n",
    "            result_dict[f\"{period}D V (%)\"] = f\"{volume_change:.2f}\" if volume_change is not None else \"nan\"\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "\n",
    "# Assuming results_df has a 'File Date' column in '%Y%m%d' format\n",
    "results_df['File Date'] = pd.to_datetime(results_df['File Date'], format='%Y%m%d')\n",
    "\n",
    "\n",
    "\n",
    "# Create a PrettyTable for displaying the results\n",
    "table = PrettyTable()\n",
    "header = [\"Stock Code\", \"Client Name\",\"Holding\",\"Percent Holding\",\"Investor Type\",\"File Date\"]\n",
    "\n",
    "for period in [1, 20, 40, 60]:\n",
    "    header.append(f\"{period}D P (%)\")\n",
    "    header.append(f\"{period}D V (%)\")\n",
    "table.field_names = header\n",
    "\n",
    "# Create a raw table to display original data\n",
    "raw_table = PrettyTable()\n",
    "raw_table.field_names = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Stock Code\"]\n",
    "\n",
    "\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    \n",
    "    # Assuming results_df has a 'File Date' column in '%Y%m%d' format\n",
    "    file_date = pd.to_datetime(row['File Date'], format='%Y%m%d')\n",
    "    \n",
    "    # Define start date\n",
    "    start_date = file_date  # Select the file date as the start date\n",
    "\n",
    "    # while True:\n",
    "    #     # Find the next day\n",
    "    start_date_pass = is_working_day_if_not_next_working_date(start_date)\n",
    "    print(start_date_pass)\n",
    "    # Calculate end date for historical data (two months later)\n",
    "    end_date = (start_date_pass + timedelta(days=120)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Download stock data for the next day\n",
    "    if isinstance(row['Code'], str):\n",
    "        stock_data = download_stock_data(row['Code'] + \".NS\", start_date_pass, end_date)\n",
    "  \n",
    "        percentage_changes = calculate_percentage_changes(stock_data, row['Code'], start_date_pass, row['Client Name'], row['Holding'], row['Percent Holding'], row['Investor Type'], file_date)\n",
    "    \n",
    "        # Add the result to the PrettyTable\n",
    "        table.add_row([percentage_changes.get(field, \"nan\") for field in table.field_names])\n",
    "\n",
    "\n",
    "print(\"\\nPercentage Changes:\")\n",
    "print(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27688802-7f60-4f01-8e64-4fa70987dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter the percentage of increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7edf8d5b-c23a-4ae2-8750-a47745da49e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Results (60D P (%) > 50):\n",
      "+------------+-------------+---------+-----------------+---------------+-----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "| Stock Code | Client Name | Holding | Percent Holding | Investor Type | File Date | 1D P (%) | 1D V (%) | 20D P (%) | 20D V (%) | 40D P (%) | 40D V (%) | 60D P (%) | 60D V (%) |\n",
      "+------------+-------------+---------+-----------------+---------------+-----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "+------------+-------------+---------+-----------------+---------------+-----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where '10D V (%)' is greater than 5\n",
    "filtered_results = [row for row in table._rows if float(row[12]) > 50]\n",
    "\n",
    "# Create a new PrettyTable for displaying the filtered results\n",
    "filtered_table = PrettyTable()\n",
    "filtered_table.field_names = table.field_names\n",
    "\n",
    "# Add the filtered rows to the new table\n",
    "for row in filtered_results:\n",
    "    filtered_table.add_row(row)\n",
    "\n",
    "# Display the filtered PrettyTable\n",
    "print(\"\\nFiltered Results (60D P (%) > 50):\")\n",
    "print(filtered_table)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc2fe4e0-6bc7-479d-ac8f-31b7ac2c5152",
   "metadata": {},
   "source": [
    "With Collective Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a5dc103-bddb-44a0-86c6-446681283a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of table: 26\n",
      "\n",
      "Filtered Results (60D P (%) > 50):\n",
      "+------------+----------------------------------------------+----------------+-----------------+---------------+---------------------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+--------+\n",
      "| Stock Code |                 Client Name                  |    Holding     | Percent Holding | Investor Type |      File Date      | 1D P (%) | 1D V (%) | 20D P (%) | 20D V (%) | 40D P (%) | 40D V (%) | 60D P (%) | 60D V (%) | 120D P (%) | 120D V (%) | Rating |\n",
      "+------------+----------------------------------------------+----------------+-----------------+---------------+---------------------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+--------+\n",
      "|   AFFLE    |                 ICICI Group                  | Sold -0.22000  |       6.4       | Institutional | 2023-10-05 00:00:00 |   0.19   |  144.51  |   -2.57   |   -9.65   |    3.22   |   130.65  |   20.04   |   232.94  |    nan     |    nan     |   2    |\n",
      "|  HDFCBANK  |                 ICICI Group                  | Bought 0.48000 |       1.84      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |    nan     |    nan     |   1    |\n",
      "|  MOLDTECH  |                Uno Metals Ltd                | Sold -0.01000  |       3.8       | Institutional | 2023-10-05 00:00:00 |   1.85   |  12.80   |   -25.29  |  1147.25  |   -30.96  |   528.40  |   -20.80  |   134.37  |    nan     |    nan     |   1    |\n",
      "|   AETHER   |            Sbi Magnum Midcap Fund            | Sold -0.02000  |       8.78      | Institutional | 2023-10-05 00:00:00 |  -0.29   |  -83.69  |   -7.60   |   -70.73  |   -18.99  |   124.49  |   -6.47   |   131.36  |    nan     |    nan     |   1    |\n",
      "| TORNTPOWER |          Axis Long Term Equity Fund          | Sold -0.61000  |       8.05      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |    nan     |    nan     |   2    |\n",
      "|  HDFCBANK  |   Icici Prudential Balanced Advantage Fund   | Bought 0.48000 |       1.84      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |    nan     |    nan     |   1    |\n",
      "|  HDFCBANK  |              Hdfc Balanced Fund              | Bought 0.26000 |       2.12      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |    nan     |    nan     |   1    |\n",
      "| MOLDTKPAC  |                SUNDARAM-GROUP                | Bought 0.07000 |       2.91      | Institutional | 2023-10-05 00:00:00 |  -0.73   |  33.62   |    0.28   |   245.77  |   -2.29   |   -23.22  |    3.69   |   67.93   |    nan     |    nan     |   1    |\n",
      "| TORNTPOWER | Hdfc Standard Life Insurance Company Limited | Bought 0.12000 |       1.26      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |    nan     |    nan     |   1    |\n",
      "|   KRSNAA   | Hdfc Standard Life Insurance Company Limited | Bought 0.16000 |       1.98      | Institutional | 2023-10-05 00:00:00 |   0.94   |  -6.41   |    5.93   |   -1.68   |   -1.33   |   -27.89  |    4.00   |   63.12   |    nan     |    nan     |   1    |\n",
      "|  ROSSARI   |                 CANARA-GROUP                 | Bought 0.31000 |       1.93      | Institutional | 2023-10-05 00:00:00 |   1.13   |  229.35  |   -13.98  |   204.50  |   -4.12   |  2449.79  |   -4.94   |   102.65  |    nan     |    nan     |   2    |\n",
      "| MOLDTKPAC  |                 CANARA-GROUP                 | Bought 0.05000 |       2.57      | Institutional | 2023-10-05 00:00:00 |  -0.73   |  33.62   |    0.28   |   245.77  |   -2.29   |   -23.22  |    3.69   |   67.93   |    nan     |    nan     |   1    |\n",
      "|   AFFLE    | Icici Prudential Life Insurance Company Ltd  | Sold -0.13000  |       3.07      | Institutional | 2023-10-05 00:00:00 |   0.19   |  144.51  |   -2.57   |   -9.65   |    3.22   |   130.65  |   20.04   |   232.94  |    nan     |    nan     |   2    |\n",
      "|  HDFCBANK  |         Hdfc Trustee Company Limited         | Bought 0.26000 |       2.12      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |    nan     |    nan     |   1    |\n",
      "| TORNTPOWER |                  Axis Group                  | Sold -0.61000  |       8.05      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |    nan     |    nan     |   2    |\n",
      "|   AETHER   |                  Axis Group                  | Sold -0.13000  |       1.26      | Institutional | 2023-10-05 00:00:00 |  -0.29   |  -83.69  |   -7.60   |   -70.73  |   -18.99  |   124.49  |   -6.47   |   131.36  |    nan     |    nan     |   1    |\n",
      "|   KRSNAA   |                 Birla Group                  | Sold -1.94000  |       2.05      | Institutional | 2023-10-05 00:00:00 |   0.94   |  -6.41   |    5.93   |   -1.68   |   -1.33   |   -27.89  |    4.00   |   63.12   |    nan     |    nan     |   2    |\n",
      "| TORNTPOWER |                  SBI Group                   | Bought 0.26000 |       5.47      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |    nan     |    nan     |   1    |\n",
      "|  ROSSARI   |                  SBI Group                   | Sold -0.02000  |       6.66      | Institutional | 2023-10-05 00:00:00 |   1.13   |  229.35  |   -13.98  |   204.50  |   -4.12   |  2449.79  |   -4.94   |   102.65  |    nan     |    nan     |   1    |\n",
      "|   AETHER   |                  SBI Group                   | Sold -0.02000  |       8.78      | Institutional | 2023-10-05 00:00:00 |  -0.29   |  -83.69  |   -7.60   |   -70.73  |   -18.99  |   124.49  |   -6.47   |   131.36  |    nan     |    nan     |   1    |\n",
      "| TORNTPOWER |                  HDFC Group                  | Bought 0.12000 |       1.26      | Institutional | 2023-10-05 00:00:00 |  -0.35   |  24.38   |    1.75   |   809.54  |   31.88   |   348.45  |   28.89   |   217.63  |    nan     |    nan     |   1    |\n",
      "|   KRSNAA   |                  HDFC Group                  | Bought 0.16000 |       1.98      | Institutional | 2023-10-05 00:00:00 |   0.94   |  -6.41   |    5.93   |   -1.68   |   -1.33   |   -27.89  |    4.00   |   63.12   |    nan     |    nan     |   1    |\n",
      "|  HDFCBANK  |                  HDFC Group                  | Bought 0.26000 |       2.12      | Institutional | 2023-10-05 00:00:00 |  -0.11   |  -63.39  |   -3.39   |   -62.70  |    5.73   |    7.66   |    8.93   |   -41.48  |    nan     |    nan     |   1    |\n",
      "| MOLDTKPAC  |                 ICICI Group                  | Bought 0.08000 |       3.41      | Institutional | 2023-10-05 00:00:00 |  -0.73   |  33.62   |    0.28   |   245.77  |   -2.29   |   -23.22  |    3.69   |   67.93   |    nan     |    nan     |   1    |\n",
      "|   KRSNAA   |                 ICICI Group                  | Bought 0.10000 |       5.4       | Institutional | 2023-10-05 00:00:00 |   0.94   |  -6.41   |    5.93   |   -1.68   |   -1.33   |   -27.89  |    4.00   |   63.12   |    nan     |    nan     |   1    |\n",
      "|  MOLDTECH  |               Akg Finvest Ltd                | Sold -0.05000  |       3.75      | Institutional | 2023-10-05 00:00:00 |   1.85   |  12.80   |   -25.29  |  1147.25  |   -30.96  |   528.40  |   -20.80  |   134.37  |    nan     |    nan     |   1    |\n",
      "+------------+----------------------------------------------+----------------+-----------------+---------------+---------------------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+--------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import math  # Import math module to check for NaN\n",
    "\n",
    "# Assuming 'table' is the original PrettyTable\n",
    "# Assuming the columns in the table are indexed as follows: \n",
    "# 0: Stock Code, 1: Client Name, 2: Holding, 3: Percent Holding, \n",
    "# 4: Investor Type, 5: File Date, 6: 1D P (%), 7: 1D V (%), \n",
    "# 8: 20D P (%), 9: 20D V (%), 10: 40D P (%), 11: 40D V (%), \n",
    "# 12: 60D P (%), 13: 60D V (%), 14: 120D P (%), 15: 120D V (%)\n",
    "\n",
    "# Copy the table to a new table\n",
    "new_table = PrettyTable()\n",
    "new_table.field_names = table.field_names\n",
    "for row in table._rows:\n",
    "    new_table.add_row(row)\n",
    "\n",
    "# Filter rows where '10D V (%)' is greater than 50 and apply the rating logic\n",
    "filtered_results = []\n",
    "print(\"Length of table:\", len(new_table._rows))\n",
    "\n",
    "for row in new_table._rows:\n",
    "    # Convert row[10] and row[12] to float if they are not NaN\n",
    "    p_40d = float(row[10])\n",
    "    p_60d = float(row[12])\n",
    "    \n",
    "    # Check if p_40d and p_60d are not NaN\n",
    "    if not math.isnan(p_40d) and not math.isnan(p_60d):\n",
    "        if 'Above' in row[2] and p_60d > 0:\n",
    "            row.append(1)\n",
    "        elif 'Above' in row[2] and p_60d < 0:\n",
    "            row.append(2)\n",
    "        elif 'Bought' in row[2] and p_60d > 0:\n",
    "            row.append(1)\n",
    "        elif 'Bought' in row[2] and p_60d < 0:\n",
    "            row.append(2)\n",
    "        elif 'Sold' in row[2] and p_60d > 0:\n",
    "            row.append(2)\n",
    "        elif 'Sold' in row[2] and p_60d < 0:\n",
    "            row.append(1)\n",
    "        elif 'below' in row[2] and p_60d > 0:\n",
    "            row.append(2)\n",
    "        elif 'below' in row[2] and p_60d < 0:\n",
    "            row.append(1)\n",
    "        else:\n",
    "            row.append(None)\n",
    "        filtered_results.append(row)\n",
    "\n",
    "# Create a new PrettyTable for displaying the filtered results\n",
    "filtered_table = PrettyTable()\n",
    "filtered_table.field_names = new_table.field_names + ['Rating']  # Append 'Rating' column\n",
    "\n",
    "# Add the filtered rows to the new table\n",
    "for row in filtered_results:\n",
    "    # Ensure the row has the same number of values as the original table\n",
    "    if len(row) == len(new_table.field_names) + 1:\n",
    "        filtered_table.add_row(row)\n",
    "\n",
    "# Display the filtered PrettyTable\n",
    "print(\"\\nFiltered Results (60D P (%) > 50):\")\n",
    "print(filtered_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8bde0ab-d736-4222-925f-d22209ad10e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated Counts:\n",
      "+----------------------------------------------+--------+-------+\n",
      "|                 Client Name                  | Rating | Times |\n",
      "+----------------------------------------------+--------+-------+\n",
      "|                 ICICI Group                  |   2    |   1   |\n",
      "|                 ICICI Group                  |   1    |   3   |\n",
      "|                Uno Metals Ltd                |   1    |   1   |\n",
      "|            Sbi Magnum Midcap Fund            |   1    |   1   |\n",
      "|          Axis Long Term Equity Fund          |   2    |   1   |\n",
      "|   Icici Prudential Balanced Advantage Fund   |   1    |   1   |\n",
      "|              Hdfc Balanced Fund              |   1    |   1   |\n",
      "|                SUNDARAM-GROUP                |   1    |   1   |\n",
      "| Hdfc Standard Life Insurance Company Limited |   1    |   2   |\n",
      "|                 CANARA-GROUP                 |   2    |   1   |\n",
      "|                 CANARA-GROUP                 |   1    |   1   |\n",
      "| Icici Prudential Life Insurance Company Ltd  |   2    |   1   |\n",
      "|         Hdfc Trustee Company Limited         |   1    |   1   |\n",
      "|                  Axis Group                  |   2    |   1   |\n",
      "|                  Axis Group                  |   1    |   1   |\n",
      "|                 Birla Group                  |   2    |   1   |\n",
      "|                  SBI Group                   |   1    |   3   |\n",
      "|                  HDFC Group                  |   1    |   3   |\n",
      "|               Akg Finvest Ltd                |   1    |   1   |\n",
      "+----------------------------------------------+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a nested defaultdict to store the counts for each client name and rating\n",
    "client_rating_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Group the filtered results by \"Client Name\" and \"Rating\" and count the occurrences\n",
    "for row in filtered_results:\n",
    "    client_name = row[1]  # Assuming \"Client Name\" is at index 1\n",
    "    rating = row[-1]  # Assuming \"Rating\" is the last element in the row\n",
    "    client_rating_counts[client_name][rating] += 1\n",
    "\n",
    "# Create a PrettyTable for displaying the counts\n",
    "count_table = PrettyTable()\n",
    "count_table.field_names = [\"Client Name\", \"Rating\", \"Times\"]\n",
    "\n",
    "# Add the counts to the table\n",
    "for client_name, ratings in client_rating_counts.items():\n",
    "    for rating, count in ratings.items():\n",
    "        count_table.add_row([client_name, rating, count])\n",
    "\n",
    "# Display the PrettyTable\n",
    "print(\"Consolidated Counts:\")\n",
    "print(count_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f74637-27b7-488a-b188-904ca7ccc13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Assuming you have defined 'table' containing the original data\n",
    "\n",
    "# Filter rows where '10D V (%)' is greater than 40 and 'Holding' column contains 'Above' or 'Bought'\n",
    "filtered_results = []\n",
    "for row in table._rows:\n",
    "    if float(row[12]) > 10 and ('Above' in row[2] or 'Bought' in row[2] ) and ('Vanguard Fund' in row[1]):\n",
    "    # if float(row[12]) > 10 and ('Vanguard Fund' in row[1]):\n",
    "    # if ('Vanguard Fund' in row[1]):\n",
    "\n",
    "        filtered_results.append(row)\n",
    "\n",
    "# Create a new PrettyTable for displaying the filtered results\n",
    "filtered_table = PrettyTable()\n",
    "filtered_table.field_names = table.field_names\n",
    "\n",
    "# Add the filtered rows to the new table\n",
    "for row in filtered_results:\n",
    "    filtered_table.add_row(row)\n",
    "\n",
    "# Display the filtered PrettyTable\n",
    "print(\"\\nFiltered Results (60D P (%) > 40 and Holding contains 'Above' or 'Bought'):\")\n",
    "print(filtered_table)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "872f2ba9-46ab-4a90-97ec-f002c3d59bd4",
   "metadata": {},
   "source": [
    "Last good working"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cda1c47-6ea4-4a75-a96b-027571cae3d3",
   "metadata": {},
   "source": [
    "Configurable File Date"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06132275-ca4f-4b3e-a570-448e636b72a1",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from prettytable import PrettyTable\n",
    "import re\n",
    "\n",
    "# Define the path to the CSV file containing 'Client Name'\n",
    "csv_path = r'C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\FII Data\\FII_Reconciliation\\FII_LIST_240128122722.csv'\n",
    "\n",
    "# Read 'Client Name' from the CSV file\n",
    "client_name_df = pd.read_csv(csv_path)\n",
    "client_names = client_name_df['Client Name'].tolist()\n",
    "\n",
    "# Define the path to the folder containing Excel files\n",
    "excel_folder_path = r'C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\OutboundSuperstars'\n",
    "\n",
    "# Define the start and end dates for the file date range\n",
    "start_date = datetime.strptime('20240124', \"%Y%m%d\")\n",
    "end_date = datetime.strptime('20240126', \"%Y%m%d\")\n",
    "\n",
    "# Create a PrettyTable to store the results\n",
    "results_table = PrettyTable()\n",
    "results_table.field_names = ['Code', 'Client Name', 'Holding', 'Percent Holding', 'Investor Type', 'File Date']\n",
    "\n",
    "# Create a list to store matching rows\n",
    "matching_rows = []\n",
    "\n",
    "# Iterate through each Excel file\n",
    "for excel_file in os.listdir(excel_folder_path):\n",
    "    excel_file_path = os.path.join(excel_folder_path, excel_file)\n",
    "\n",
    "    # Get the list of sheet names in the Excel file\n",
    "    sheet_names = pd.ExcelFile(excel_file_path).sheet_names\n",
    "\n",
    "    # Check if 'Shareholding' sheet exists\n",
    "    if 'Shareholding' in sheet_names:\n",
    "        # Read the 'Shareholding' sheet\n",
    "        df = pd.read_excel(excel_file_path, sheet_name='Shareholding', skiprows=6)\n",
    "\n",
    "        # Extract date from the file name using regular expression\n",
    "        match = re.search(r'\\d{8}', excel_file)  # Assuming the date in the file name is in YYYYMMDD format\n",
    "        if match:\n",
    "            file_date_str = match.group(0)\n",
    "            file_date = datetime.strptime(file_date_str, \"%Y%m%d\")\n",
    "\n",
    "            # Check if the file date is within the specified range\n",
    "            if start_date <= file_date <= end_date:\n",
    "                # Check if 'Client Name' is present in the DataFrame\n",
    "                if 'Client Name' in df.columns:\n",
    "                    # Check if each 'Client Name' from the CSV is present in the Excel file\n",
    "                    present_in_excel = [client_name in df['Client Name'].values for client_name in client_names]\n",
    "\n",
    "                    # Display only the specified columns for matched records\n",
    "                    for i, present in enumerate(present_in_excel):\n",
    "                        if present:\n",
    "                            matching_row = df[df['Client Name'] == client_names[i]]\n",
    "                            matching_row = matching_row[['Code', 'Client Name', 'Holding', 'Percent Holding', 'Investor Type']]\n",
    "                            matching_row['File Date'] = file_date_str\n",
    "                            matching_rows.append(matching_row)\n",
    "\n",
    "# Concatenate all matching rows into a DataFrame\n",
    "results_df = pd.concat(matching_rows)\n",
    "\n",
    "# Sort the DataFrame by 'File Date' in descending order\n",
    "results_df = results_df.sort_values(by='File Date', ascending=False)\n",
    "\n",
    "# Print the results using PrettyTable\n",
    "for _, row in results_df.iterrows():\n",
    "    results_table.add_row(row.tolist())\n",
    "\n",
    "# Display the PrettyTable\n",
    "print(results_table)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3a1a736-f247-4c3b-b31e-9f8f93b5af3b",
   "metadata": {},
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def download_stock_data(symbol, start_date, end_date):\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "def calculate_percentage_changes(data, stock_code, start_date):\n",
    "    result_dict = {\"Stock Code\": stock_code}\n",
    "\n",
    "    # Calculate percentage changes for the next day, one week, two weeks, one month, and two months\n",
    "    periods = [1, 5, 10, 20, 40]\n",
    "    for period in periods:\n",
    "        if len(data) >= period + 1:\n",
    "            print(period)\n",
    "            print(data['Close'].loc[start_date])\n",
    "            print(data['Close'].iloc[period])\n",
    "            close_change_1d = ((data['Close'].loc[start_date] - data['Close'].iloc[period]) / data['Close'].loc[start_date]) * 100\n",
    "            volume_change = ((data['Volume'].loc[start_date] - data['Volume'].iloc[period]) / data['Volume'].loc[start_date]) * 100\n",
    "        else:\n",
    "            close_change_1d = volume_change = None\n",
    "\n",
    "        result_dict[f\"{period}D P (%)\"] = f\"{close_change_1d:.2f}\" if close_change_1d is not None else \"nan\"\n",
    "        result_dict[f\"{period}D V (%)\"] = f\"{volume_change:.2f}\" if volume_change is not None else \"nan\"\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "# List of stock symbols for NSE\n",
    "symbols_list = ['RBLBANK.NS']\n",
    "\n",
    "# Define start date\n",
    "start_date = '2024-01-05'\n",
    "\n",
    "# Calculate end date for historical data (two months later)\n",
    "end_date = (datetime.strptime(start_date, \"%Y-%m-%d\") + timedelta(days=60)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Create a PrettyTable for displaying the results\n",
    "table = PrettyTable()\n",
    "header = [\"Stock Code\"]\n",
    "for period in [1, 5, 10, 20, 40]:\n",
    "    header.append(f\"{period}D P (%)\")\n",
    "    header.append(f\"{period}D V (%)\")\n",
    "table.field_names = header\n",
    "\n",
    "# Create a raw table to display original data\n",
    "raw_table = PrettyTable()\n",
    "raw_table.field_names = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Stock Code\"]\n",
    "\n",
    "# Iterate over each stock symbol\n",
    "for symbol in symbols_list:\n",
    "    # Extract stock code without the extension\n",
    "    stock_code = symbol.split('.')[0]\n",
    "\n",
    "    # Download historical stock data using the function\n",
    "    stock_data = download_stock_data(symbol, start_date, end_date)\n",
    "\n",
    "    # Calculate percentage changes\n",
    "    percentage_changes = calculate_percentage_changes(stock_data, stock_code, start_date)\n",
    "\n",
    "    # Add the result to the PrettyTable\n",
    "    table.add_row([percentage_changes.get(field, \"nan\") for field in table.field_names])\n",
    "\n",
    "    # Add original data to the raw table\n",
    "    for _, row in stock_data.iterrows():\n",
    "        raw_table.add_row([row.name.strftime(\"%Y-%m-%d\"), row[\"Open\"], row[\"High\"], row[\"Low\"], row[\"Close\"],\n",
    "                           row[\"Adj Close\"], row[\"Volume\"], stock_code])\n",
    "\n",
    "# Print the PrettyTable\n",
    "print(\"\\nPercentage Changes:\")\n",
    "print(table)\n",
    "\n",
    "# Print the Raw Table\n",
    "print(\"\\nRaw Table:\")\n",
    "print(raw_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a180b-8c89-48a4-8e7a-0a50e592dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "import re\n",
    "\n",
    "# Define the path to the CSV file containing 'Client Name'\n",
    "csv_path = r'C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\FII Data\\FII_Reconciliation\\FII_LIST_240128122722.csv'\n",
    "\n",
    "# Read 'Client Name' from the CSV file\n",
    "client_name_df = pd.read_csv(csv_path)\n",
    "client_names = client_name_df['Client Name'].tolist()\n",
    "\n",
    "\n",
    "# Define the path to the folder containing Excel files\n",
    "excel_folder_path = r'C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\OutboundSuperstars'\n",
    "\n",
    "# Create a PrettyTable to store the results\n",
    "results_table = PrettyTable()\n",
    "results_table.field_names = ['Code', 'Client Name', 'Holding', 'Percent Holding', 'Investor Type', 'File Date']\n",
    "\n",
    "# Create a list to store matching rows\n",
    "matching_rows = []\n",
    "\n",
    "# Iterate through each Excel file\n",
    "for excel_file in os.listdir(excel_folder_path):\n",
    "    excel_file_path = os.path.join(excel_folder_path, excel_file)\n",
    "\n",
    "    # Get the list of sheet names in the Excel file\n",
    "    sheet_names = pd.ExcelFile(excel_file_path).sheet_names\n",
    "\n",
    "    # Check if 'Shareholding' sheet exists\n",
    "    if 'Shareholding' in sheet_names:\n",
    "        # Read the 'Shareholding' sheet\n",
    "        df = pd.read_excel(excel_file_path, sheet_name='Shareholding', skiprows=6)\n",
    "\n",
    "        # Extract date from the file name using regular expression\n",
    "        match = re.search(r'\\d{8}', excel_file)  # Assuming the date in the file name is in YYYYMMDD format\n",
    "        if match:\n",
    "            file_date = match.group(0)\n",
    "        else:\n",
    "            file_date = 'Not Found'\n",
    "\n",
    "        # Check if 'Client Name' is present in the DataFrame\n",
    "        if 'Client Name' in df.columns:\n",
    "            # Check if each 'Client Name' from the CSV is present in the Excel file\n",
    "            present_in_excel = [client_name in df['Client Name'].values for client_name in client_names]\n",
    "\n",
    "            # Display only the specified columns for matched records\n",
    "            for i, present in enumerate(present_in_excel):\n",
    "                if present:\n",
    "                    matching_row = df[df['Client Name'] == client_names[i]]\n",
    "                    matching_row = matching_row[['Code', 'Client Name', 'Holding', 'Percent Holding', 'Investor Type']]\n",
    "                    matching_row['File Date'] = file_date\n",
    "                    matching_rows.append(matching_row)\n",
    "\n",
    "# Concatenate all matching rows into a DataFrame\n",
    "results_df = pd.concat(matching_rows)\n",
    "\n",
    "# Sort the DataFrame by 'File Date' in descending order\n",
    "results_df = results_df.sort_values(by='File Date', ascending=False)\n",
    "\n",
    "# Print the results using PrettyTable\n",
    "for _, row in results_df.iterrows():\n",
    "    results_table.add_row(row.tolist())\n",
    "\n",
    "# Display the PrettyTable\n",
    "print(results_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb57ae1-f8e5-409e-bd22-3e2641b6c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from pymongo import MongoClient\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='news_analysis.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# MongoDB connection URI (replace with your actual URI)\n",
    "uri = \"mongodb://localhost:27017\"\n",
    "\n",
    "# Specify the database and collection\n",
    "database_name = \"NewsAnalytics\"\n",
    "collection_name = \"RawNews_Hindu\"\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_file_path = r'C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Scrip\\Scrip01022024.xlsx'\n",
    "\n",
    "try:\n",
    "    # Create a new client and connect to the MongoDB server\n",
    "    with MongoClient(uri) as client:\n",
    "        collection = client[database_name][collection_name]\n",
    "\n",
    "        # Query all records in the collection\n",
    "        start_date = datetime(2024, 2, 2, 0, 0, 0)\n",
    "        end_date = datetime(2024, 2, 3, 0, 0, 0)\n",
    "        find_query = {\n",
    "            \"created_at\": {\n",
    "                \"$gte\": start_date,\n",
    "                \"$lt\": end_date\n",
    "            }\n",
    "        }\n",
    "\n",
    "        all_records = collection.find(find_query)\n",
    "        mongo_documents = list(all_records)\n",
    "\n",
    "        if not mongo_documents:\n",
    "            logging.warning(\"No records found from DB\")\n",
    "            print(\"No records found from DB\")\n",
    "        else:\n",
    "            # Read the \"Portfolio\" sheet from the Excel file\n",
    "            portfolio_df = pd.read_excel(excel_file_path, sheet_name='Top1000NSE', header=0)\n",
    "\n",
    "            # Assuming the column containing stock names is named \"Top1000NSE\"\n",
    "            stock_names_in_portfolio = portfolio_df['Top1000NSE'].tolist()\n",
    "\n",
    "            # Create a list to store tuples of (Scrip, paragraph_content, label, confidence, date, strength)\n",
    "            result_list = []\n",
    "\n",
    "            # Check if each stock name in the portfolio is present in MongoDB documents\n",
    "            for document in mongo_documents:\n",
    "                for stock_name in stock_names_in_portfolio:\n",
    "                    if stock_name.lower() in document['paragraph_content'].lower():\n",
    "                        # Wrap paragraph content to a maximum of five words per line\n",
    "                        wrapped_content = '\\n'.join(textwrap.wrap(document['paragraph_content'], width=110))\n",
    "                        # Extract the numeric part from the 'Score' column\n",
    "                        numeric_score = int(''.join(filter(str.isdigit, document['sentiment']['label'])))\n",
    "\n",
    "                        result_list.append((\n",
    "                            stock_name,\n",
    "                            wrapped_content,\n",
    "                            numeric_score,  # Use the extracted numeric score\n",
    "                            document['sentiment']['confidence'],\n",
    "                            document['created_at'].strftime('%Y-%m-%d'),  # Extract date from created_at\n",
    "                            document['FinBertScore'],\n",
    "                            None  # Placeholder for Strength column\n",
    "                        ))\n",
    "                        break  # Break if at least one match is found for the current document\n",
    "\n",
    "            # Create a DataFrame from the result list\n",
    "            result_df = pd.DataFrame(result_list, columns=['Scrip', 'Paragraph_content', 'Score', 'Confidence', 'Date', 'FinBertScore', 'Strength'])\n",
    "\n",
    "            # Check if Positive/Negative sheet is present in the Excel file\n",
    "            xls = pd.ExcelFile(excel_file_path, engine='openpyxl')  # Use openpyxl engine\n",
    "            if 'PositiveNegative' in xls.sheet_names:\n",
    "                # Read the Positive/Negative sheet from the Excel file\n",
    "                positive_negative_df = pd.read_excel(xls, sheet_name='PositiveNegative', header=0)\n",
    "\n",
    "                # Assuming the column containing positive/negative words is named \"PositiveNegative\"\n",
    "                positive_negative_words = positive_negative_df['PositiveNegative'].tolist()\n",
    "\n",
    "                # Update the Strength column based on positive/negative words\n",
    "                result_df['Strength'] = result_df.apply(\n",
    "                    lambda row: ', '.join(\n",
    "                        positive_negative_word\n",
    "                        for positive_negative_word in positive_negative_words\n",
    "                        if positive_negative_word.lower() in row['Paragraph_content'].lower()\n",
    "                    ),\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                # Wrap the entire 'Strength' column to a maximum width of 140 characters\n",
    "                result_df['Strength'] = result_df['Strength'].apply(lambda x: '\\n'.join(textwrap.wrap(str(x), width=17)))\n",
    "\n",
    "            # Sort the DataFrame based on multiple columns in ascending order\n",
    "            result_df['Confidence'] = result_df['Confidence'].apply(lambda x: f\"{x:.5f}\")\n",
    "            result_df = result_df.sort_values(by=['Date', 'Scrip', 'Score', 'Confidence'], ascending=[False, True, False, False])\n",
    "\n",
    "            # Display the DataFrame\n",
    "            print(tabulate(result_df, headers='keys', tablefmt='pretty'))\n",
    "\n",
    "        \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error: {e}\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d9217-de16-4c52-bbad-67f99202c4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96817b-30c8-4ac5-8b04-9f4d2548fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a DataFrame with features and labels for the first dataset\n",
    "data1 = {\n",
    "    'Date': ['2021-01-01', '2021-01-02', '2021-01-03'],\n",
    "    'Stock': ['ABC', 'XYZ', 'ABC'],\n",
    "    'FinBertScore': [1, 2, 0],\n",
    "    'PriceMovement': [1, 0, 1]  # 1 for increase, 0 for decrease\n",
    "}\n",
    "\n",
    "# Assuming you have a DataFrame for the second dataset\n",
    "data2 = {\n",
    "    'Date': ['2021-01-01', '2021-01-02'],\n",
    "    'Stock': ['ABC', 'XYZ'],\n",
    "    'FII Name': ['XYZ FII', 'ABC FII'],\n",
    "    'FII Action': ['Buy 3%', 'Sell 2%'],\n",
    "    'Future Price Movement Weekly': ['+1%', '-0.5%'],\n",
    "    'Future Price Movement Monthly': ['+4%', '-2%']\n",
    "}\n",
    "\n",
    "# Create DataFrames from the provided data\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Feature selection for the first dataset\n",
    "features1 = ['FinBertScore']\n",
    "target1 = 'PriceMovement'\n",
    "\n",
    "# Feature selection for the second dataset\n",
    "features2 = ['FII Action']\n",
    "target2 = 'Future Price Movement Weekly'\n",
    "\n",
    "# Merge the datasets on 'Date' and 'Stock'\n",
    "df_combined = pd.merge(df1, df2, on=['Date', 'Stock'], how='inner')\n",
    "\n",
    "# Check if the merged DataFrame is not empty\n",
    "if df_combined.empty:\n",
    "    print(\"Merged DataFrame is empty. Check merging columns and common values.\")\n",
    "else:\n",
    "    # Extract numerical information from 'FII Action' column\n",
    "    df_combined['FII Action'] = df_combined['FII Action'].apply(lambda x: re.search(r'\\d+', x).group()).astype(float)\n",
    "\n",
    "    # Create features and labels for the combined dataset\n",
    "    X_combined = df_combined[features1 + features2]\n",
    "    y_combined = df_combined[target1]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(\n",
    "        X_combined, y_combined, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "    # Convert NumPy arrays back to DataFrames\n",
    "    X_train_combined = pd.DataFrame(X_train_combined, columns=X_combined.columns, index=y_train_combined.index)\n",
    "    X_test_combined = pd.DataFrame(X_test_combined, columns=X_combined.columns, index=y_test_combined.index)\n",
    "    \n",
    "    # Choose a classification algorithm (Random Forest in this example)\n",
    "    model_combined = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Train the model on the combined dataset\n",
    "    model_combined.fit(X_train_combined, y_train_combined)\n",
    "    \n",
    "    # Make predictions on the combined test set\n",
    "    y_pred_combined = model_combined.predict(X_test_combined)\n",
    "    \n",
    "    # Map predicted values to Buy (1) or Sell (0)\n",
    "    buy_sell_predictions_combined = ['Buy' if pred == 1 else 'Sell' for pred in y_pred_combined]\n",
    "    \n",
    "    # Create a new column 'Buy/Sell' in the combined test set\n",
    "    df_test_combined = df_combined.loc[X_test_combined.index].copy()\n",
    "    df_test_combined['Buy/Sell'] = buy_sell_predictions_combined\n",
    "    \n",
    "\n",
    "    # Display the DataFrame with Buy/Sell column for the combined dataset in a pretty table\n",
    "    table = PrettyTable()\n",
    "    table.field_names = df_test_combined.columns\n",
    "    \n",
    "    for _, row in df_test_combined.iterrows():\n",
    "        table.add_row(row)\n",
    "    \n",
    "    print(\"Pretty Table with Buy/Sell for the combined dataset:\")\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "633c1a8c-c68b-4015-a378-3e6e3f5d270a",
   "metadata": {},
   "source": [
    "# import yfinance as yf\n",
    "# from datetime import datetime, timedelta\n",
    "# from prettytable import PrettyTable\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import time\n",
    "\n",
    "# # Define the path to the CSV file containing 'Client Name'\n",
    "# csv_path = r'C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\Institutional_Investor_Data\\Individual_Investor_LIST_240130164809.csv'\n",
    "\n",
    "# # Read 'Client Name' from the CSV file\n",
    "# client_name_df = pd.read_csv(csv_path)\n",
    "# client_names = client_name_df['Client Name'].tolist()\n",
    "\n",
    "# # Define the path to the folder containing Excel files\n",
    "# excel_folder_path = r'C:\\Users\\91908\\Documents\\Raja\\Share market\\Analysis\\Trendlyne\\Data\\OutboundSuperstars'\n",
    "\n",
    "# # Define the start and end dates for the file date range\n",
    "# start_date = datetime.strptime('20231001', \"%Y%m%d\")\n",
    "# end_date = datetime.strptime('20231005', \"%Y%m%d\")\n",
    "\n",
    "# # Create a PrettyTable to store the results\n",
    "# results_table = PrettyTable()\n",
    "# results_table.field_names = ['Code', 'Client Name', 'Holding', 'Percent Holding', 'Investor Type', 'File Date']\n",
    "\n",
    "# # Create a list to store matching rows\n",
    "# matching_rows = []\n",
    "\n",
    "# # Iterate through each Excel file\n",
    "# for excel_file in os.listdir(excel_folder_path):\n",
    "#     excel_file_path = os.path.join(excel_folder_path, excel_file)\n",
    "\n",
    "#     # Get the list of sheet names in the Excel file\n",
    "#     sheet_names = pd.ExcelFile(excel_file_path).sheet_names\n",
    "\n",
    "#     # Check if 'Shareholding' sheet exists\n",
    "#     if 'Shareholding' in sheet_names:\n",
    "#         # Read the 'Shareholding' sheet\n",
    "#         df = pd.read_excel(excel_file_path, sheet_name='Shareholding', skiprows=6)\n",
    "\n",
    "#         # Extract date from the file name using regular expression\n",
    "#         match = re.search(r'\\d{8}', excel_file)  # Assuming the date in the file name is in YYYYMMDD format\n",
    "#         if match:\n",
    "#             file_date_str = match.group(0)\n",
    "#             file_date = datetime.strptime(file_date_str, \"%Y%m%d\")\n",
    "\n",
    "#             # Check if the file date is within the specified range\n",
    "#             if start_date <= file_date <= end_date:\n",
    "#                 # Check if 'Client Name' is present in the DataFrame\n",
    "#                 if 'Client Name' in df.columns:\n",
    "#                     # Check if each 'Client Name' from the CSV is present in the Excel file\n",
    "#                     present_in_excel = [client_name in df['Client Name'].values for client_name in client_names]\n",
    "\n",
    "#                     # Display only the specified columns for matched records\n",
    "#                     for i, present in enumerate(present_in_excel):\n",
    "#                         if present:\n",
    "#                             matching_row = df[df['Client Name'] == client_names[i]]\n",
    "#                             matching_row = matching_row[['Code', 'Client Name', 'Holding', 'Percent Holding', 'Investor Type']]\n",
    "#                             matching_row['File Date'] = file_date_str\n",
    "#                             matching_rows.append(matching_row)\n",
    "\n",
    "# # Concatenate all matching rows into a DataFrame\n",
    "# results_df = pd.concat(matching_rows)\n",
    "\n",
    "# # Sort the DataFrame by 'File Date' in descending order\n",
    "# results_df = results_df.sort_values(by='File Date', ascending=False)\n",
    "\n",
    "# # Print the results using PrettyTable\n",
    "# for _, row in results_df.iterrows():\n",
    "#     results_table.add_row(row.tolist())\n",
    "\n",
    "# # Display the PrettyTable\n",
    "# # print(results_table)\n",
    "\n",
    "# def download_stock_data(symbol, start_date, end_date):\n",
    "#     data = yf.download(symbol, start=start_date, end=end_date)\n",
    "#     return data\n",
    "\n",
    "# def calculate_percentage_changes(data, stock_code, start_date,client_name,holding,percent_holding,investor_type,file_date ):\n",
    "#     result_dict = {\n",
    "#         \"Stock Code\": stock_code,\n",
    "#         \"Client Name\": client_name,  # Corrected variable name\n",
    "#         \"Holding\": holding,  # Corrected variable name\n",
    "#         \"Percent Holding\": percent_holding,  # Corrected variable name\n",
    "#         \"Investor Type\": investor_type,  # Corrected variable name\n",
    "#         \"File Date\": file_date,  # Corrected variable name\n",
    "#     }\n",
    "\n",
    "#     # Calculate percentage changes for the next day, one week, two weeks, one month, and two months\n",
    "#     # periods = [1, 5, 10, 20, 40]\n",
    "#     # for period in periods:\n",
    "#     #     if len(data) >= period + 1:\n",
    "#     #         print(period)\n",
    "#     #         print(stock_code)\n",
    "#     #         print(file_date)\n",
    "#     #         print(start_date)\n",
    "#     #         print(data['Close'].loc[start_date])\n",
    "#     #         print(data['Close'].iloc[period])\n",
    "#     #         # close_change_1d = ((data['Close'].loc[start_date] - data['Close'].iloc[period]) / data['Close'].loc[start_date]) * 100\n",
    "#     #         # volume_change = ((data['Volume'].loc[start_date] - data['Volume'].iloc[period]) / data['Volume'].loc[start_date]) * 100\n",
    "#     #         close_change_1d = ((data['Close'].iloc[period] - data['Close'].loc[start_date] ) / data['Close'].loc[start_date]) * 100\n",
    "#     #         volume_change = ((data['Volume'].iloc[period] - data['Volume'].loc[start_date] ) / data['Volume'].loc[start_date]) * 100\n",
    "#     #     else:\n",
    "#     #     close_change_1d = volume_change = None\n",
    "\n",
    "#     #     result_dict[f\"{period}D P (%)\"] = f\"{close_change_1d:.2f}\" if close_change_1d is not None else \"nan\"\n",
    "#     #     result_dict[f\"{period}D V (%)\"] = f\"{volume_change:.2f}\" if volume_change is not None else \"nan\"\n",
    "#     # # print(result_dict)\n",
    "#     # return result_dict\n",
    "\n",
    "#     periods = [1, 20, 40, 60, 120]\n",
    "\n",
    "#     for period in periods:\n",
    "#          if len(data) >= period + 1:\n",
    "\n",
    "#             close_change_1d = volume_change = None\n",
    "    \n",
    "#             # Check if the start date is in the index\n",
    "#             if start_date in data.index:\n",
    "#                 close_change_1d = ((data['Close'].iloc[period] - data['Close'].loc[start_date]) / data['Close'].loc[start_date]) * 100\n",
    "#                 volume_change = ((data['Volume'].iloc[period] - data['Volume'].loc[start_date]) / data['Volume'].loc[start_date]) * 100\n",
    "    \n",
    "#             result_dict[f\"{period}D P (%)\"] = f\"{close_change_1d:.2f}\" if close_change_1d is not None else \"nan\"\n",
    "#             result_dict[f\"{period}D V (%)\"] = f\"{volume_change:.2f}\" if volume_change is not None else \"nan\"\n",
    "\n",
    "#     return result_dict\n",
    "# # List of stock symbols for NSE\n",
    "# # symbols_list = ['IDEAFORGE.NS', 'INFY.NS']\n",
    "\n",
    "# # # Define start date\n",
    "# # #start_date = '2023-12-22'\n",
    "# # results_df['File Date'] = pd.to_datetime(results_df['File Date'], format='%Y%m%d').dt.strftime('%Y-%m-%d')\n",
    "# # start_date=results_df['File Date']\n",
    "# # print(start_date)\n",
    "# # # Calculate end date for historical data (two months later)\n",
    "# # # end_date = (datetime.strptime(start_date, \"%Y-%m-%d\") + timedelta(days=60)).strftime(\"%Y-%m-%d\")\n",
    "# # end_date = (start_date + timedelta(days=60)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming results_df has a 'File Date' column in '%Y%m%d' format\n",
    "# results_df['File Date'] = pd.to_datetime(results_df['File Date'], format='%Y%m%d')\n",
    "\n",
    "#         # # Define start date\n",
    "#         # start_date = results_df['File Date'].iloc[0]  # Select the first row, you may adjust as needed\n",
    "#         # # print(\"start_date\")\n",
    "#         # # print(start_date)\n",
    "        \n",
    "#         # # Calculate end date for historical data (two months later)\n",
    "#         # end_date = (start_date + timedelta(days=90)).strftime(\"%Y-%m-%d\")\n",
    "#         # # print(\"end_date\")\n",
    "#         # # print(end_date)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Create a PrettyTable for displaying the results\n",
    "# table = PrettyTable()\n",
    "# header = [\"Stock Code\", \"Client Name\",\"Holding\",\"Percent Holding\",\"Investor Type\",\"File Date\"]\n",
    "\n",
    "# for period in [1, 20, 40, 60, 120]:\n",
    "#     header.append(f\"{period}D P (%)\")\n",
    "#     header.append(f\"{period}D V (%)\")\n",
    "# table.field_names = header\n",
    "\n",
    "# # Create a raw table to display original data\n",
    "# raw_table = PrettyTable()\n",
    "# raw_table.field_names = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"Stock Code\"]\n",
    "\n",
    "\n",
    "\n",
    "# for _, row in results_df.iterrows():\n",
    "#     # symbol = row['Code']\n",
    "    \n",
    "#     # Assuming results_df has a 'File Date' column in '%Y%m%d' format\n",
    "#     file_date = pd.to_datetime(row['File Date'], format='%Y%m%d')\n",
    "    \n",
    "#     # Define start date\n",
    "#     start_date = file_date  # Select the file date as the start date\n",
    "#     # print(f\"Processing {symbol} for {start_date}...\")\n",
    "\n",
    "#     # Calculate end date for historical data (two months later)\n",
    "#     end_date = (start_date + timedelta(days=140)).strftime(\"%Y-%m-%d\")\n",
    "#     # print(f\"Processing {symbol} for {start_date}...\")\n",
    "\n",
    "#     # # Extract stock code without the extension\n",
    "#     # stock_code = symbol.split('.')[0]\n",
    "#     # print(symbol.split('.')[0])\n",
    "#     # print(stock_code)\n",
    "#     max_attempts = 4\n",
    "#     # Initialize attempts counter\n",
    "#     attempts = 0\n",
    "    \n",
    "#     while True:\n",
    "#         # Find the next day\n",
    "#         start_date_pass = start_date.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "#         # Calculate end date for historical data (two months later)\n",
    "#         end_date = (start_date + timedelta(days=140)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "#         # Download stock data for the next day\n",
    "#         stock_data = download_stock_data(str(row['Code']) + \".NS\", start_date_pass, end_date)\n",
    "    \n",
    "#         # Increment attempts\n",
    "#         attempts += 1\n",
    "    \n",
    "#         # Check the exit condition\n",
    "#         if start_date_pass in stock_data.index or attempts >= max_attempts:\n",
    "#             # print(f\"data found for {row['Code']} on {start_date_pass}.\")\n",
    "#             break\n",
    "    \n",
    "#     # Calculate percentage changes\n",
    "#     if start_date_pass in stock_data.index:\n",
    "#         percentage_changes = calculate_percentage_changes(stock_data, row['Code'], start_date_pass, row['Client Name'], row['Holding'], row['Percent Holding'], row['Investor Type'], file_date)\n",
    "    \n",
    "#         # Add the result to the PrettyTable\n",
    "#         table.add_row([percentage_changes.get(field, \"nan\") for field in table.field_names])\n",
    "    \n",
    "    \n",
    "#     else:\n",
    "#         print(f\"No data found for {row['Code']} on {start_date_pass}.\")\n",
    "# # Print the PrettyTable\n",
    "# print(\"\\nPercentage Changes:\")\n",
    "# print(table)\n",
    "# # # Iterate over each stock symbol\n",
    "# # for _, row in results_df.iterrows():\n",
    "# #     symbol = row['Code']\n",
    "# #     # print(row['Client Name'])\n",
    "# #     # print(row['Holding'])\n",
    "# #     # print(row['Percent Holding'])\n",
    "# #     # print(row['Investor Type'])\n",
    "# #     # print(row['File Date'])\n",
    "\n",
    "# #     # Assuming results_df has a 'File Date' column in '%Y%m%d' format\n",
    "# #     results_df['File Date'] = pd.to_datetime(row['File Date'], format='%Y%m%d')\n",
    "    \n",
    "# #     # Define start date\n",
    "# #     start_date = results_df['File Date'].iloc[0]  # Select the first row, you may adjust as needed\n",
    "# #     # print(\"Inside loop start_date\")\n",
    "# #     # print(start_date)\n",
    "    \n",
    "# #     # Calculate end date for historical data (two months later)\n",
    "# #     end_date = (start_date + timedelta(days=60)).strftime(\"%Y-%m-%d\")\n",
    "# #     # print(\"Inside loop end_date\")\n",
    "# #     # print(end_date)\n",
    "\n",
    "\n",
    "    \n",
    "# #     # Extract stock code without the extension\n",
    "# #     stock_code = symbol.split('.')[0]\n",
    "\n",
    "# #     # Download historical stock data using the function\n",
    "    \n",
    "# #     stock_data = download_stock_data(symbol+\".NS\", start_date, end_date)\n",
    "# #     # print(stock_data)\n",
    "# #     # Calculate percentage changes\n",
    "# #     # percentage_changes = calculate_percentage_changes(stock_data, stock_code, start_date)\n",
    "# #     percentage_changes = calculate_percentage_changes(stock_data, stock_code, start_date, row['Client Name'],row['Holding'], row['Percent Holding'],row['Investor Type'],row['File Date'])\n",
    "\n",
    "# #     # Add the result to the PrettyTable\n",
    "# #     table.add_row([percentage_changes.get(field, \"nan\") for field in table.field_names])\n",
    "\n",
    "# #     # Add original data to the raw table\n",
    "# #     for _, row in stock_data.iterrows():\n",
    "# #         raw_table.add_row([row.name.strftime(\"%Y-%m-%d\"), row[\"Open\"], row[\"High\"], row[\"Low\"], row[\"Close\"],\n",
    "# #                            row[\"Adj Close\"], row[\"Volume\"], stock_code])\n",
    "\n",
    "# # # Print the PrettyTable\n",
    "# # print(\"\\nPercentage Changes:\")\n",
    "# # print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e430b5-6703-4f8b-be16-676e87cdd45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
